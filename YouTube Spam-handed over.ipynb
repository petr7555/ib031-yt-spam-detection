{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Detection in YouTube Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#nlp #classification\n",
    "\n",
    "Your task is to classify YouTube comments into spam and ham categories. You need to use something other than bag of words and Naive Bayes as your models (but it can be a baseline).\n",
    "\n",
    "Dataset source: https://archive.ics.uci.edu/ml/datasets/YouTube+Spam+Collection.\n",
    "\n",
    "Assignment: https://is.muni.cz/auth/el/fi/jaro2021/IB031/um/cviceni/assignment.pdf?predmet=1323750"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/petr.janik/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Math and data stuff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB, ComplementNB, BernoulliNB, MultinomialNB, CategoricalNB\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Tensorflow Keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GaussianNoise, LSTM, Bidirectional, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Language stuff\n",
    "from pymagnitude import Magnitude, MagnitudeUtils\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Other\n",
    "import html\n",
    "from mlxtend.feature_selection import ColumnSelector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "\n",
    "def load_all_data(drop_duplicates=True):\n",
    "    data_dir = \"data\"\n",
    "\n",
    "    psy = pd.read_csv(f\"{data_dir}/Youtube01-Psy.csv\", parse_dates=[\"DATE\"])\n",
    "    katy = pd.read_csv(f\"{data_dir}/Youtube02-KatyPerry.csv\", parse_dates=[\"DATE\"])\n",
    "    lmfao = pd.read_csv(f\"{data_dir}/Youtube03-LMFAO.csv\", parse_dates=[\"DATE\"])\n",
    "    eminem = pd.read_csv(f\"{data_dir}/Youtube04-Eminem.csv\", parse_dates=[\"DATE\"])\n",
    "    shakira = pd.read_csv(f\"{data_dir}/Youtube05-Shakira.csv\", parse_dates=[\"DATE\"])\n",
    "    \n",
    "    all_datasets = [psy, katy, lmfao, eminem, shakira]\n",
    "    dataset_names = [\"psy\", \"katy\", \"lmfao\", \"eminem\", \"shakira\"]\n",
    "\n",
    "    # keep info about which video the comment appeared in\n",
    "    for dataset_name, dataset in zip(dataset_names, all_datasets):\n",
    "        dataset[\"INTERPRET\"] = dataset_name\n",
    "\n",
    "    # join all datasets\n",
    "    joined = pd.concat(all_datasets).reset_index(drop=True)\n",
    "    \n",
    "    # common preprocessing\n",
    "    if drop_duplicates:\n",
    "        joined.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # convert object types to strings\n",
    "    object_cols = joined.select_dtypes(\"object\").columns\n",
    "    joined[object_cols] = joined[object_cols].astype(\"string\")\n",
    "    \n",
    "    return joined\n",
    "\n",
    "def load_data():\n",
    "    all_data = load_all_data()\n",
    "    \n",
    "    df, final_test_df = train_test_split(\n",
    "        all_data, test_size=0.2, random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_final_test_data():\n",
    "    all_data = load_all_data()\n",
    "    \n",
    "    df, final_test_df = train_test_split(\n",
    "        all_data, test_size=0.2, random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    return final_test_df\n",
    "\n",
    "def load_train_test_all_cols_data(test_size=0.2):\n",
    "    df = load_data()\n",
    "    df_X, df_y = df.drop(columns=\"CLASS\"), df.CLASS\n",
    "    \n",
    "    return train_test_split(\n",
    "        df_X, df_y, test_size=test_size, random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "def load_train_test_data():\n",
    "    df = load_data()\n",
    "    df_X, df_y = df.CONTENT, df.CLASS\n",
    "    \n",
    "    return train_test_split(\n",
    "        df_X, df_y, test_size=0.2, random_state=RANDOM_STATE\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data\"\n",
    "\n",
    "psy = pd.read_csv(f\"{data_dir}/Youtube01-Psy.csv\", parse_dates=[\"DATE\"])\n",
    "katy = pd.read_csv(f\"{data_dir}/Youtube02-KatyPerry.csv\", parse_dates=[\"DATE\"])\n",
    "lmfao = pd.read_csv(f\"{data_dir}/Youtube03-LMFAO.csv\", parse_dates=[\"DATE\"])\n",
    "eminem = pd.read_csv(f\"{data_dir}/Youtube04-Eminem.csv\", parse_dates=[\"DATE\"])\n",
    "shakira = pd.read_csv(f\"{data_dir}/Youtube05-Shakira.csv\", parse_dates=[\"DATE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presentation: https://docs.google.com/presentation/d/1GIoRrhQ2_eERuowLotWscQhx7X4s4A4mwBm3-aGpO_E/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset comes from 2017.\n",
    "It has five datasets composed by 1 956 real messages extracted from five videos that were among the 10 most viewed on the collection period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Artist     | Song (our guess)     | Song published on YouTube (date) | Comments range |\n",
    "|------------|----------------------|----------------------------------|----------------|\n",
    "| Psy        | Gangnam Style        | 15. 7. 2012                      | 2013–2015      |\n",
    "| Katy Perry | Roar                 | 5. 9. 2013                       | 2014–2015      |\n",
    "| LMFAO      | Party Rock Anthem    | 9. 3. 2011                       | 2014–2015      |\n",
    "| Eminem     | Love The Way You Lie | 5. 8. 2010                       | 2015-2015      |\n",
    "| Shakira    | Waka Waka            | 5. 6. 2010                       | 2013–2015      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07 06:20:48.000</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A</td>\n",
       "      <td>adam riyati</td>\n",
       "      <td>2013-11-07 12:37:15.000</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8</td>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>2013-11-08 17:34:21.000</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z13jhp0bxqncu512g22wvzkasxmvvzjaz04</td>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>2013-11-09 08:28:43.000</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z13fwbwp1oujthgqj04chlngpvzmtt3r3dw</td>\n",
       "      <td>GsMega</td>\n",
       "      <td>2013-11-10 16:05:38.000</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>_2viQ_Qnc6-bMSjqyL1NKj57ROicCSJV5SwTrw-RFFA</td>\n",
       "      <td>Katie Mettam</td>\n",
       "      <td>2013-07-13 13:27:39.441</td>\n",
       "      <td>I love this song because we sing it at Camp al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>_2viQ_Qnc6-pY-1yR6K2FhmC5i48-WuNx5CumlHLDAI</td>\n",
       "      <td>Sabina Pearson-Smith</td>\n",
       "      <td>2013-07-13 13:14:30.021</td>\n",
       "      <td>I love this song for two reasons: 1.it is abou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>_2viQ_Qnc6_k_n_Bse9zVhJP8tJReZpo8uM2uZfnzDs</td>\n",
       "      <td>jeffrey jules</td>\n",
       "      <td>2013-07-13 12:09:31.188</td>\n",
       "      <td>wow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>_2viQ_Qnc6_yBt8UGMWyg3vh0PulTqcqyQtdE7d4Fl0</td>\n",
       "      <td>Aishlin Maciel</td>\n",
       "      <td>2013-07-13 11:17:52.308</td>\n",
       "      <td>Shakira u are so wiredo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>_2viQ_Qnc685RPw1aSa1tfrIuHXRvAQ2rPT9R06KTqA</td>\n",
       "      <td>Latin Bosch</td>\n",
       "      <td>2013-07-12 22:33:27.916</td>\n",
       "      <td>Shakira is the best dancer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1956 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       COMMENT_ID                AUTHOR  \\\n",
       "0     LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU             Julius NM   \n",
       "1     LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A           adam riyati   \n",
       "2     LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8      Evgeny Murashkin   \n",
       "3             z13jhp0bxqncu512g22wvzkasxmvvzjaz04       ElNino Melendez   \n",
       "4             z13fwbwp1oujthgqj04chlngpvzmtt3r3dw                GsMega   \n",
       "...                                           ...                   ...   \n",
       "1951  _2viQ_Qnc6-bMSjqyL1NKj57ROicCSJV5SwTrw-RFFA          Katie Mettam   \n",
       "1952  _2viQ_Qnc6-pY-1yR6K2FhmC5i48-WuNx5CumlHLDAI  Sabina Pearson-Smith   \n",
       "1953  _2viQ_Qnc6_k_n_Bse9zVhJP8tJReZpo8uM2uZfnzDs         jeffrey jules   \n",
       "1954  _2viQ_Qnc6_yBt8UGMWyg3vh0PulTqcqyQtdE7d4Fl0        Aishlin Maciel   \n",
       "1955  _2viQ_Qnc685RPw1aSa1tfrIuHXRvAQ2rPT9R06KTqA           Latin Bosch   \n",
       "\n",
       "                        DATE  \\\n",
       "0    2013-11-07 06:20:48.000   \n",
       "1    2013-11-07 12:37:15.000   \n",
       "2    2013-11-08 17:34:21.000   \n",
       "3    2013-11-09 08:28:43.000   \n",
       "4    2013-11-10 16:05:38.000   \n",
       "...                      ...   \n",
       "1951 2013-07-13 13:27:39.441   \n",
       "1952 2013-07-13 13:14:30.021   \n",
       "1953 2013-07-13 12:09:31.188   \n",
       "1954 2013-07-13 11:17:52.308   \n",
       "1955 2013-07-12 22:33:27.916   \n",
       "\n",
       "                                                CONTENT  CLASS  \n",
       "0     Huh, anyway check out this you[tube] channel: ...      1  \n",
       "1     Hey guys check out my new channel and our firs...      1  \n",
       "2                just for test I have to say murdev.com      1  \n",
       "3      me shaking my sexy ass on my channel enjoy ^_^ ﻿      1  \n",
       "4               watch?v=vtaRGgvGtWQ   Check this out .﻿      1  \n",
       "...                                                 ...    ...  \n",
       "1951  I love this song because we sing it at Camp al...      0  \n",
       "1952  I love this song for two reasons: 1.it is abou...      0  \n",
       "1953                                                wow      0  \n",
       "1954                            Shakira u are so wiredo      0  \n",
       "1955                         Shakira is the best dancer      0  \n",
       "\n",
       "[1956 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined = pd.concat([psy, katy, lmfao, eminem, shakira]).reset_index(drop=True)\n",
    "joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1956 total rows in the dataset.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(joined)} total rows in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column `CLASS` is the label column we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 1005 spams and 951 hams.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"There are {len(joined[joined.CLASS == 1])} spams and {len(joined[joined.CLASS == 0])} hams.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the data set at the very beginning not to make the test data dirty.\n",
    "`final_test_X` and `final_test_y` will be used at the very end to evaluate and compare our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_X, joined_y = joined.drop(columns=\"CLASS\"), joined.CLASS\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "train_X_orig, final_test_X, train_y, final_test_y = train_test_split(\n",
    "    joined_X, joined_y, test_size=0.2, random_state=random_state\n",
    ")\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    joined, test_size=0.2, random_state=random_state\n",
    ")\n",
    "\n",
    "train_df = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After putting test data apart, there are 1564 train data rows.\n"
     ]
    }
   ],
   "source": [
    "print(f\"After putting test data apart, there are {len(train_X_orig)} train data rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 789 spams and 775 hams.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"There are {(train_y == 1).sum()} spams and {(train_y == 0).sum()} hams.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...which is well balanced, so we do not need any fancy sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1564 entries, 836 to 1126\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   COMMENT_ID  1564 non-null   object        \n",
      " 1   AUTHOR      1564 non-null   object        \n",
      " 2   DATE        1369 non-null   datetime64[ns]\n",
      " 3   CONTENT     1564 non-null   object        \n",
      "dtypes: datetime64[ns](1), object(3)\n",
      "memory usage: 61.1+ KB\n"
     ]
    }
   ],
   "source": [
    "train_X_orig.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset includes the following columns:\n",
    "- COMMENT_ID - string\n",
    "- AUTHOR - string\n",
    "- DATE - date\n",
    "- CONTENT (comment itself) - string\n",
    "- CLASS (1 = spam, 0 = ham) - categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column `COMMENT_ID` provides very little (no) value, let's drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>Blaze Blaziken</td>\n",
       "      <td>2015-05-20 03:50:29.098</td>\n",
       "      <td>you cant stop the shuffle﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1688</th>\n",
       "      <td>Lucia Scarlet</td>\n",
       "      <td>2015-05-22 11:56:53.104</td>\n",
       "      <td>Amazing song﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>Paul Buxton</td>\n",
       "      <td>2015-05-21 11:12:22.066</td>\n",
       "      <td>Omg! This guy sounds like an american professo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1650</th>\n",
       "      <td>Tierra Carbon</td>\n",
       "      <td>2015-05-24 14:21:54.411</td>\n",
       "      <td>It was  cool   the best   song ever  ﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573</th>\n",
       "      <td>David Bottenberg</td>\n",
       "      <td>NaT</td>\n",
       "      <td>subscribe to my channel  /watch?v=NxK32i0HkDs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>ItsJoey Dash</td>\n",
       "      <td>2014-07-22 10:04:05.755</td>\n",
       "      <td>EVERYONE PLEASE SUBSCRIBE TO MY CHANNEL OR CAN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>louis canellony</td>\n",
       "      <td>NaT</td>\n",
       "      <td>watch youtube video &amp;quot;EMINEM -YTMA artist ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>vieshva .d.exodous</td>\n",
       "      <td>2015-05-18 08:38:34.236</td>\n",
       "      <td>Awesome﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>viviane trinh</td>\n",
       "      <td>2015-05-21 22:35:35.753</td>\n",
       "      <td>i like the lyrics but not to music video﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>Juan Martinez</td>\n",
       "      <td>2014-07-22 13:22:54.111</td>\n",
       "      <td>I learned the shuffle because of them﻿</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1564 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  AUTHOR                    DATE  \\\n",
       "836       Blaze Blaziken 2015-05-20 03:50:29.098   \n",
       "1688       Lucia Scarlet 2015-05-22 11:56:53.104   \n",
       "1505         Paul Buxton 2015-05-21 11:12:22.066   \n",
       "1650       Tierra Carbon 2015-05-24 14:21:54.411   \n",
       "1573    David Bottenberg                     NaT   \n",
       "...                  ...                     ...   \n",
       "1130        ItsJoey Dash 2014-07-22 10:04:05.755   \n",
       "1294     louis canellony                     NaT   \n",
       "860   vieshva .d.exodous 2015-05-18 08:38:34.236   \n",
       "1459       viviane trinh 2015-05-21 22:35:35.753   \n",
       "1126       Juan Martinez 2014-07-22 13:22:54.111   \n",
       "\n",
       "                                                CONTENT  \n",
       "836                          you cant stop the shuffle﻿  \n",
       "1688                                      Amazing song﻿  \n",
       "1505  Omg! This guy sounds like an american professo...  \n",
       "1650             It was  cool   the best   song ever  ﻿  \n",
       "1573      subscribe to my channel  /watch?v=NxK32i0HkDs  \n",
       "...                                                 ...  \n",
       "1130  EVERYONE PLEASE SUBSCRIBE TO MY CHANNEL OR CAN...  \n",
       "1294  watch youtube video &quot;EMINEM -YTMA artist ...  \n",
       "860                                            Awesome﻿  \n",
       "1459          i like the lyrics but not to music video﻿  \n",
       "1126             I learned the shuffle because of them﻿  \n",
       "\n",
       "[1564 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = train_X_orig.drop(columns=\"COMMENT_ID\")\n",
    "train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>The `AUTHOR` column can help in classifying spam – spammers may have specific names (e.g. having a \"normal\" name instead of a username, copying a well-known username etc.).</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no correlations between columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis**: Something that looks like a spam is not always a spam, it depends on the video.\n",
    "E.g. if the word appears in the video subtitles, it is less likely to be a spam. However, we will not develop this hypothesis any further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration: Missing values? (Eminem is missing a lot of dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "psy\n",
      "\tCOMMENT_ID: NO missing values\n",
      "\tAUTHOR    : NO missing values\n",
      "\tDATE      : NO missing values\n",
      "\tCONTENT   : NO missing values\n",
      "\tCLASS     : NO missing values\n",
      "katy\n",
      "\tCOMMENT_ID: NO missing values\n",
      "\tAUTHOR    : NO missing values\n",
      "\tDATE      : NO missing values\n",
      "\tCONTENT   : NO missing values\n",
      "\tCLASS     : NO missing values\n",
      "lmfao\n",
      "\tCOMMENT_ID: NO missing values\n",
      "\tAUTHOR    : NO missing values\n",
      "\tDATE      : NO missing values\n",
      "\tCONTENT   : NO missing values\n",
      "\tCLASS     : NO missing values\n",
      "eminem\n",
      "\tCOMMENT_ID: NO missing values\n",
      "\tAUTHOR    : NO missing values\n",
      "\tDATE      : missing values\n",
      "\tCONTENT   : NO missing values\n",
      "\tCLASS     : NO missing values\n",
      "shakira\n",
      "\tCOMMENT_ID: NO missing values\n",
      "\tAUTHOR    : NO missing values\n",
      "\tDATE      : NO missing values\n",
      "\tCONTENT   : NO missing values\n",
      "\tCLASS     : NO missing values\n"
     ]
    }
   ],
   "source": [
    "for dataset, dataset_name in zip([psy, katy, lmfao, eminem, shakira], [\"psy\", \"katy\", \"lmfao\", \"eminem\", \"shakira\"]):\n",
    "    na = dataset.isna().any()\n",
    "    print(dataset_name)\n",
    "    for missing, column_name in zip(na, na.index):\n",
    "        print(f'\\t{column_name:10}: {\"\" if missing else \"NO \"}missing values')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that only eminem dataset has missing valued and they are in the `DATE` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'245 out of 448 DATE values in eminem dataset are missing.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{eminem.DATE.isna().sum()} out of {len(eminem)} DATE values in eminem dataset are missing.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we wanted to replace `NaN` dates with previous `non NaN` date in a sorted dataset, but..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    245\n",
       "Name: CLASS, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eminem[eminem.DATE.isna()][\"CLASS\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>All Eminem comments that do not have DATE are spams. The information that DATE is missing is therefore valuable.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration: What range are comment dates in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "psy: 2013-2015\n",
      "katy: 2014-2015\n",
      "lmfao: 2014-2015\n",
      "eminem: 2015-2015\n",
      "shakira: 2013-2015\n"
     ]
    }
   ],
   "source": [
    "for dataset, dataset_name in zip([psy, katy, lmfao, eminem, shakira], [\"psy\", \"katy\", \"lmfao\", \"eminem\", \"shakira\"]):\n",
    "    sorted_dataset = dataset[~dataset.DATE.isna()].DATE.sort_values()\n",
    "    earliest = sorted_dataset.iloc[0].year\n",
    "    latest = sorted_dataset.iloc[-1].year\n",
    "    print(f\"{dataset_name}: {earliest}-{latest}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis: Longer comment implies spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6304347826086957"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = train_X.CONTENT.str.len() >= 50\n",
    "accuracy_score(train_y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...better than random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis: Spams contain \"check\" (e.g. \"Check out my channel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5818414322250639"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = train_X.CONTENT.str.contains(\"check\")\n",
    "accuracy_score(train_y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...better than random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration: Are there any duplicate comments based on ID?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1421</th>\n",
       "      <td>LneaDw26bFvPh9xBHNw1btQoyP60ay_WWthtvXCx37s</td>\n",
       "      <td>janez novak</td>\n",
       "      <td>NaT</td>\n",
       "      <td>share and like this page to win a hand signed ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>LneaDw26bFuH6iFsSrjlJLJIX3qD4R8-emuZ-aGUj0o</td>\n",
       "      <td>Amir bassem</td>\n",
       "      <td>NaT</td>\n",
       "      <td>if u love rihanna subscribe me</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>LneaDw26bFvPh9xBHNw1btQoyP60ay_WWthtvXCx37s</td>\n",
       "      <td>janez novak</td>\n",
       "      <td>NaT</td>\n",
       "      <td>share and like this page to win a hand signed ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>_2viQ_Qnc68fX3dYsfYuM-m4ELMJvxOQBmBOFHqGOk0</td>\n",
       "      <td>tyler sleetway</td>\n",
       "      <td>2013-10-05 00:57:25.078</td>\n",
       "      <td>so beutiful</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>_2viQ_Qnc68fX3dYsfYuM-m4ELMJvxOQBmBOFHqGOk0</td>\n",
       "      <td>tyler sleetway</td>\n",
       "      <td>2013-10-05 00:57:25.078</td>\n",
       "      <td>so beutiful</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>LneaDw26bFuH6iFsSrjlJLJIX3qD4R8-emuZ-aGUj0o</td>\n",
       "      <td>Amir bassem</td>\n",
       "      <td>NaT</td>\n",
       "      <td>if u love rihanna subscribe me</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       COMMENT_ID          AUTHOR  \\\n",
       "1421  LneaDw26bFvPh9xBHNw1btQoyP60ay_WWthtvXCx37s     janez novak   \n",
       "1441  LneaDw26bFuH6iFsSrjlJLJIX3qD4R8-emuZ-aGUj0o     Amir bassem   \n",
       "1420  LneaDw26bFvPh9xBHNw1btQoyP60ay_WWthtvXCx37s     janez novak   \n",
       "1797  _2viQ_Qnc68fX3dYsfYuM-m4ELMJvxOQBmBOFHqGOk0  tyler sleetway   \n",
       "1798  _2viQ_Qnc68fX3dYsfYuM-m4ELMJvxOQBmBOFHqGOk0  tyler sleetway   \n",
       "1443  LneaDw26bFuH6iFsSrjlJLJIX3qD4R8-emuZ-aGUj0o     Amir bassem   \n",
       "\n",
       "                        DATE  \\\n",
       "1421                     NaT   \n",
       "1441                     NaT   \n",
       "1420                     NaT   \n",
       "1797 2013-10-05 00:57:25.078   \n",
       "1798 2013-10-05 00:57:25.078   \n",
       "1443                     NaT   \n",
       "\n",
       "                                                CONTENT  CLASS  \n",
       "1421  share and like this page to win a hand signed ...      1  \n",
       "1441                     if u love rihanna subscribe me      1  \n",
       "1420  share and like this page to win a hand signed ...      1  \n",
       "1797                                        so beutiful      0  \n",
       "1798                                        so beutiful      0  \n",
       "1443                     if u love rihanna subscribe me      1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df.COMMENT_ID.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>We want to drop duplicate rows.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We lose only 3 rows by dropping duplicates.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"We lose only {len(train_df) - len(train_df.drop_duplicates())} rows by dropping duplicates.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there any duplicate comments based on Author name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>z13xtdlovm2hzl05d04ccz1pnvqtezdriqc0k</td>\n",
       "      <td>Uroš Slemenjak</td>\n",
       "      <td>2014-11-07 12:08:13.000</td>\n",
       "      <td>People, here is a new network like FB...you re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>z12wj5g52rzbcvprl04cenuj1yyifhxq3hw</td>\n",
       "      <td>LuckyMusiqLive</td>\n",
       "      <td>2014-09-15 17:47:57.000</td>\n",
       "      <td>Katy has the voice of gold. this video really ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>z12udxjwpwurtlwz304ccbrhdtusth4herk0k</td>\n",
       "      <td>PacKmaN</td>\n",
       "      <td>2014-11-05 21:56:39.000</td>\n",
       "      <td>check men out i put allot of effort into my mu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>_2viQ_Qnc6_umVgV0fI-CSScDHuFxNHIVvezCGhajW8</td>\n",
       "      <td>Alain Bruno</td>\n",
       "      <td>2013-10-02 04:01:20.922</td>\n",
       "      <td>Shakira is very beautiful</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131</th>\n",
       "      <td>z12dw3tbbzm2gpzty22gtf1bvviqeha2j</td>\n",
       "      <td>ItsJoey Dash</td>\n",
       "      <td>2014-07-22 10:04:00.700</td>\n",
       "      <td>EVERYONE PLEASE SUBSCRIBE TO MY CHANNEL OR CAN...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>z13jvndqyr2zjzpxo04cij4pdva4yhqzlq40k</td>\n",
       "      <td>D Maw</td>\n",
       "      <td>2015-05-25 05:43:54.048</td>\n",
       "      <td>Remeber when this song was good﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>LneaDw26bFuUfz6WhRf9xiRHsxHm0t4fXYLbcPhB7Lk</td>\n",
       "      <td>Scott Johnson</td>\n",
       "      <td>NaT</td>\n",
       "      <td>You guys should check out this EXTRAORDINARY w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>z132svd4fvq1wntfd221w5szfzezjri2r</td>\n",
       "      <td>Abdullah Fawzi</td>\n",
       "      <td>2015-05-25 06:23:24.405</td>\n",
       "      <td>see this&lt;br /&gt;&lt;a href=\"http://adf.ly\"&gt;http://a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>z13qfn5yusqoslnn222dutdw4yqmhzhej</td>\n",
       "      <td>LiveLikeLien x</td>\n",
       "      <td>2015-05-20 00:57:56.444</td>\n",
       "      <td>It makes me happy instantly, and makes me forg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>z121szzyozr4vpqqc04cdn5g4zjhutdosdw</td>\n",
       "      <td>ItsJoey Dash</td>\n",
       "      <td>2014-07-22 10:04:05.755</td>\n",
       "      <td>EVERYONE PLEASE SUBSCRIBE TO MY CHANNEL OR CAN...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>184 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       COMMENT_ID          AUTHOR  \\\n",
       "192         z13xtdlovm2hzl05d04ccz1pnvqtezdriqc0k  Uroš Slemenjak   \n",
       "462           z12wj5g52rzbcvprl04cenuj1yyifhxq3hw  LuckyMusiqLive   \n",
       "141         z12udxjwpwurtlwz304ccbrhdtusth4herk0k         PacKmaN   \n",
       "1828  _2viQ_Qnc6_umVgV0fI-CSScDHuFxNHIVvezCGhajW8     Alain Bruno   \n",
       "1131            z12dw3tbbzm2gpzty22gtf1bvviqeha2j    ItsJoey Dash   \n",
       "...                                           ...             ...   \n",
       "769         z13jvndqyr2zjzpxo04cij4pdva4yhqzlq40k           D Maw   \n",
       "1332  LneaDw26bFuUfz6WhRf9xiRHsxHm0t4fXYLbcPhB7Lk   Scott Johnson   \n",
       "1638            z132svd4fvq1wntfd221w5szfzezjri2r  Abdullah Fawzi   \n",
       "1724            z13qfn5yusqoslnn222dutdw4yqmhzhej  LiveLikeLien x   \n",
       "1130          z121szzyozr4vpqqc04cdn5g4zjhutdosdw    ItsJoey Dash   \n",
       "\n",
       "                        DATE  \\\n",
       "192  2014-11-07 12:08:13.000   \n",
       "462  2014-09-15 17:47:57.000   \n",
       "141  2014-11-05 21:56:39.000   \n",
       "1828 2013-10-02 04:01:20.922   \n",
       "1131 2014-07-22 10:04:00.700   \n",
       "...                      ...   \n",
       "769  2015-05-25 05:43:54.048   \n",
       "1332                     NaT   \n",
       "1638 2015-05-25 06:23:24.405   \n",
       "1724 2015-05-20 00:57:56.444   \n",
       "1130 2014-07-22 10:04:05.755   \n",
       "\n",
       "                                                CONTENT  CLASS  \n",
       "192   People, here is a new network like FB...you re...      1  \n",
       "462   Katy has the voice of gold. this video really ...      1  \n",
       "141   check men out i put allot of effort into my mu...      1  \n",
       "1828                          Shakira is very beautiful      0  \n",
       "1131  EVERYONE PLEASE SUBSCRIBE TO MY CHANNEL OR CAN...      1  \n",
       "...                                                 ...    ...  \n",
       "769                    Remeber when this song was good﻿      0  \n",
       "1332  You guys should check out this EXTRAORDINARY w...      1  \n",
       "1638  see this<br /><a href=\"http://adf.ly\">http://a...      1  \n",
       "1724  It makes me happy instantly, and makes me forg...      0  \n",
       "1130  EVERYONE PLEASE SUBSCRIBE TO MY CHANNEL OR CAN...      1  \n",
       "\n",
       "[184 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_authors = train_df[train_df.duplicated(subset=[\"AUTHOR\"], keep=False)]\n",
    "duplicate_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>z13xtdlovm2hzl05d04ccz1pnvqtezdriqc0k</td>\n",
       "      <td>Uroš Slemenjak</td>\n",
       "      <td>2014-11-07 12:08:13</td>\n",
       "      <td>People, here is a new network like FB...you re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>z13ocdbaxwqdvjnwx04ccz1pnvqtezdriqc0k</td>\n",
       "      <td>Uroš Slemenjak</td>\n",
       "      <td>2014-11-07 12:16:21</td>\n",
       "      <td>People, here is a new network like FB...you re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                COMMENT_ID          AUTHOR  \\\n",
       "192  z13xtdlovm2hzl05d04ccz1pnvqtezdriqc0k  Uroš Slemenjak   \n",
       "658  z13ocdbaxwqdvjnwx04ccz1pnvqtezdriqc0k  Uroš Slemenjak   \n",
       "\n",
       "                   DATE                                            CONTENT  \\\n",
       "192 2014-11-07 12:08:13  People, here is a new network like FB...you re...   \n",
       "658 2014-11-07 12:16:21  People, here is a new network like FB...you re...   \n",
       "\n",
       "     CLASS  \n",
       "192      1  \n",
       "658      1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df.AUTHOR == \"Uroš Slemenjak\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    149\n",
       "0     35\n",
       "Name: CLASS, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_authors[\"CLASS\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'80.98% of comments from authors who posted multiple comments are spams.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spams, hams = duplicate_authors[\"CLASS\"].value_counts()\n",
    "f\"{(spams / (spams + hams)) * 100:.02f}% of comments from authors who posted multiple comments are spams.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the same author post both spams and hams? If so, are the labels truly correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 0 authors who posted both spam and ham. But this might not be generally '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_by_author = duplicate_authors.groupby(\"AUTHOR\").CLASS.mean()\n",
    "posted_both_spam_and_ham = (grouped_by_author != 0) & (grouped_by_author != 1)\n",
    "f\"There are {len(grouped_by_author[posted_both_spam_and_ham])} authors who posted both spam and ham. But this might not be generally \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 0 comments that are classified both as spam and ham.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_comments = train_df[train_df.duplicated(subset=[\"CONTENT\"], keep=False)]\n",
    "grouped_by_content = duplicate_comments.groupby(\"CONTENT\").CLASS.mean()\n",
    "same_content_spam_and_ham = (grouped_by_content != 0) & (grouped_by_content != 1)\n",
    "f\"There are {len(grouped_by_content[same_content_spam_and_ham])} comments that are classified both as spam and ham.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this might be true only for the data we are working with, and not truth in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>z13xxf3qlq2bxpm1o22zidpqbn2tfpcjr04</td>\n",
       "      <td>Connor Mire</td>\n",
       "      <td>2015-05-21 20:46:54.704</td>\n",
       "      <td>This Song is AWESOME!!!!﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>z13uzb3zqoylw1kl022zidpqbn2tfpcjr04</td>\n",
       "      <td>Connor Mire</td>\n",
       "      <td>2015-05-21 20:47:08.673</td>\n",
       "      <td>I&amp;#39;m A SUBSCRIBER﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              COMMENT_ID       AUTHOR                    DATE  \\\n",
       "810  z13xxf3qlq2bxpm1o22zidpqbn2tfpcjr04  Connor Mire 2015-05-21 20:46:54.704   \n",
       "809  z13uzb3zqoylw1kl022zidpqbn2tfpcjr04  Connor Mire 2015-05-21 20:47:08.673   \n",
       "\n",
       "                       CONTENT  CLASS  \n",
       "810  This Song is AWESOME!!!!﻿      0  \n",
       "809      I&#39;m A SUBSCRIBER﻿      1  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[test_df.AUTHOR == \"Connor Mire\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that *Connor Mire* posted both spam and ham. And the labels are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'77.30% of duplicate comments are spams.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spams, hams = duplicate_comments[\"CLASS\"].value_counts()\n",
    "f\"{(spams / (spams + hams)) * 100:.02f}% of duplicate comments are spams.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take advantage of custom transformers. This way, it is easy to use them in pipelines both for training and testing. We can easily select a subset of transformations and vary transformations based on used model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = load_train_test_all_cols_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>INTERPRET</th>\n",
       "      <th>HAS_LINK</th>\n",
       "      <th>NOT_UNIQUE_AUTHOR</th>\n",
       "      <th>NULL_IN_DATE_TIME</th>\n",
       "      <th>SUSPICIOUS_WORDS_COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>z13wxtdpeznid12et23ogtd4zoyvzbnoz04</td>\n",
       "      <td>Sonny Carter</td>\n",
       "      <td>2015-05-22 11:46:35.988</td>\n",
       "      <td>I love this song sooooooooooooooo much﻿</td>\n",
       "      <td>eminem</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1846</th>\n",
       "      <td>_2viQ_Qnc6-adCzTDLAhqNVQ5hFYcjPyPI5m7pHY4BY</td>\n",
       "      <td>Lizzy Molly</td>\n",
       "      <td>2013-09-09 17:34:07.052</td>\n",
       "      <td>PLEASE CHECK OUT MY VIDEO CALLED &amp;quot;WE LOVE...</td>\n",
       "      <td>shakira</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>z13uutbriumnuj3rq04ccbvqlwjuj1srhyk0k</td>\n",
       "      <td>Warcorpse666</td>\n",
       "      <td>2015-05-26 02:27:43.254</td>\n",
       "      <td>sorry but eminmem is a worthless wife beating ...</td>\n",
       "      <td>eminem</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>z121gbuy2unhc5m4n04cf3kyslqhepeqgvo0k</td>\n",
       "      <td>Santeri Saariokari</td>\n",
       "      <td>2014-09-03 16:32:59.000</td>\n",
       "      <td>Hey guys go to check my video name \"growtopia ...</td>\n",
       "      <td>katy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>z133hdqrqpukup0lp22chhoaztrhvxov5</td>\n",
       "      <td>Quinho Divulgaçoes</td>\n",
       "      <td>2014-11-06 19:50:16.000</td>\n",
       "      <td>me segue ha  https://www.facebook.com/marcos.s...</td>\n",
       "      <td>katy</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1865</th>\n",
       "      <td>_2viQ_Qnc6_FlLJN0izQaKVQNe6LGDmPZMmkVDjjymE</td>\n",
       "      <td>Neeru bala</td>\n",
       "      <td>2013-09-05 23:07:09.056</td>\n",
       "      <td>Hi.. Everyone.. If anyone after real online wo...</td>\n",
       "      <td>shakira</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>LneaDw26bFsltJodWnZAafXscqrATBuKDM8-8lA4TQE</td>\n",
       "      <td>miamiscraziest</td>\n",
       "      <td>NaT</td>\n",
       "      <td>LADIES!!! -----&amp;gt;&amp;gt; If you have a broken h...</td>\n",
       "      <td>eminem</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>z12sitjpgyy3f5j2322iy1figqa4vnyja04</td>\n",
       "      <td>OverSpace33</td>\n",
       "      <td>2014-11-06 19:40:59.000</td>\n",
       "      <td>For Christmas Song visit my channel! ;)﻿</td>\n",
       "      <td>psy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>z13axbnqtxfrc3ncc23xxp2wivqbgx43o</td>\n",
       "      <td>sahil samal</td>\n",
       "      <td>2014-11-08 06:28:01.000</td>\n",
       "      <td>1 millioon dislikessssssssssssssssssssssssssss...</td>\n",
       "      <td>psy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>z13mznp52kzciz12t22xx1q5unnxxd1ih04</td>\n",
       "      <td>michael smith</td>\n",
       "      <td>2014-10-30 05:20:18.000</td>\n",
       "      <td>00 : 39 Im pretty sure that tiger just wanted ...</td>\n",
       "      <td>katy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1249 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       COMMENT_ID              AUTHOR  \\\n",
       "1447          z13wxtdpeznid12et23ogtd4zoyvzbnoz04        Sonny Carter   \n",
       "1846  _2viQ_Qnc6-adCzTDLAhqNVQ5hFYcjPyPI5m7pHY4BY         Lizzy Molly   \n",
       "1304        z13uutbriumnuj3rq04ccbvqlwjuj1srhyk0k        Warcorpse666   \n",
       "402         z121gbuy2unhc5m4n04cf3kyslqhepeqgvo0k  Santeri Saariokari   \n",
       "652             z133hdqrqpukup0lp22chhoaztrhvxov5  Quinho Divulgaçoes   \n",
       "...                                           ...                 ...   \n",
       "1865  _2viQ_Qnc6_FlLJN0izQaKVQNe6LGDmPZMmkVDjjymE          Neeru bala   \n",
       "1427  LneaDw26bFsltJodWnZAafXscqrATBuKDM8-8lA4TQE      miamiscraziest   \n",
       "172           z12sitjpgyy3f5j2322iy1figqa4vnyja04         OverSpace33   \n",
       "269             z13axbnqtxfrc3ncc23xxp2wivqbgx43o         sahil samal   \n",
       "625           z13mznp52kzciz12t22xx1q5unnxxd1ih04       michael smith   \n",
       "\n",
       "                        DATE  \\\n",
       "1447 2015-05-22 11:46:35.988   \n",
       "1846 2013-09-09 17:34:07.052   \n",
       "1304 2015-05-26 02:27:43.254   \n",
       "402  2014-09-03 16:32:59.000   \n",
       "652  2014-11-06 19:50:16.000   \n",
       "...                      ...   \n",
       "1865 2013-09-05 23:07:09.056   \n",
       "1427                     NaT   \n",
       "172  2014-11-06 19:40:59.000   \n",
       "269  2014-11-08 06:28:01.000   \n",
       "625  2014-10-30 05:20:18.000   \n",
       "\n",
       "                                                CONTENT INTERPRET  HAS_LINK  \\\n",
       "1447            I love this song sooooooooooooooo much﻿    eminem         0   \n",
       "1846  PLEASE CHECK OUT MY VIDEO CALLED &quot;WE LOVE...   shakira         0   \n",
       "1304  sorry but eminmem is a worthless wife beating ...    eminem         0   \n",
       "402   Hey guys go to check my video name \"growtopia ...      katy         0   \n",
       "652   me segue ha  https://www.facebook.com/marcos.s...      katy         2   \n",
       "...                                                 ...       ...       ...   \n",
       "1865  Hi.. Everyone.. If anyone after real online wo...   shakira         0   \n",
       "1427  LADIES!!! -----&gt;&gt; If you have a broken h...    eminem         0   \n",
       "172            For Christmas Song visit my channel! ;)﻿       psy         0   \n",
       "269   1 millioon dislikessssssssssssssssssssssssssss...       psy         0   \n",
       "625   00 : 39 Im pretty sure that tiger just wanted ...      katy         0   \n",
       "\n",
       "      NOT_UNIQUE_AUTHOR  NULL_IN_DATE_TIME  SUSPICIOUS_WORDS_COUNT  \n",
       "1447                  0                  0                       0  \n",
       "1846                  0                  0                       3  \n",
       "1304                  0                  0                       0  \n",
       "402                   0                  0                       3  \n",
       "652                   0                  0                       0  \n",
       "...                 ...                ...                     ...  \n",
       "1865                  0                  0                       0  \n",
       "1427                  0                  2                       3  \n",
       "172                   0                  0                       0  \n",
       "269                   0                  0                       0  \n",
       "625                   0                  0                       0  \n",
       "\n",
       "[1249 rows x 9 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ExplorativeTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.duplicated_and_spammers = 0\n",
    "        self.duplicated_and_not_spammers = 0\n",
    "        self.not_unique_authors = set()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        \n",
    "        stop = stopwords.words('english')\n",
    "\n",
    "        author_set = set()\n",
    "        for author in X[\"AUTHOR\"]:\n",
    "            if author in author_set:\n",
    "                self.not_unique_authors.add(author)\n",
    "            author_set.add(author)\n",
    "            \n",
    "        # are all duplicated authors spammers?\n",
    "        for index, row in X.iterrows():\n",
    "            if row[\"AUTHOR\"] in self.not_unique_authors and y[index] == 1:\n",
    "                self.duplicated_and_spammers += 1\n",
    "            if row[\"AUTHOR\"] in self.not_unique_authors and y[index] == 0:\n",
    "                self.duplicated_and_not_spammers += 1\n",
    "        \n",
    "        \"\"\"\n",
    "        Creates 2 dictionaries: spam_dic and ham_dict, one that counts occurence of each word in spam comments, \n",
    "        other counts occurence of each word in ham comments.\n",
    "        Further, only those words that are repeated more than 15 times are left in ham_dict, \n",
    "        and those repeated more than 35 times in spam_dict.\n",
    "        Numbers are chosen like that, because I wanted to empahsise, that it is more importatnt to not mark as spam sth.,\n",
    "        that is not spam, than the other way around.\n",
    "        The last step is to keep only those words in spam_dict that are not in ham_dict and are not stop_words.\n",
    "        \"\"\"\n",
    "\n",
    "        self.spam_dict = ExplorativeTransformer.get_suspicious_words(X, y, 1)\n",
    "        ham_dict = ExplorativeTransformer.get_suspicious_words(X, y, 0)\n",
    "        my_inverted_dict_spam = dict(map(reversed, self.spam_dict.items()))\n",
    "        my_inverted_dict_ham = dict(map(reversed, ham_dict.items()))\n",
    "        suspicious_spam = []\n",
    "        suspicious_ham = []\n",
    "\n",
    "        for key in my_inverted_dict_spam:\n",
    "            if key > 15 and my_inverted_dict_spam[key] not in stop:\n",
    "                suspicious_spam.append(my_inverted_dict_spam[key])\n",
    "\n",
    "        for key in my_inverted_dict_ham:\n",
    "            if key > 35 and my_inverted_dict_ham[key] not in stop:\n",
    "                suspicious_ham.append(my_inverted_dict_ham[key])\n",
    "\n",
    "        # remove all from spam that is in ham\n",
    "        self.suspicious_words_list = []\n",
    "        for word in suspicious_spam:\n",
    "            if word not in suspicious_ham:\n",
    "                self.suspicious_words_list.append(word)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        X[\"HAS_LINK\"] = np.where(X['CONTENT'].str.contains('http') |\n",
    "                                  X['CONTENT'].str.contains('//'), 2, 0)\n",
    "    \n",
    "        X[\"NOT_UNIQUE_AUTHOR\"] = np.where(X['AUTHOR'].str in self.not_unique_authors, self.duplicated_and_not_spammers\n",
    "                                           // self.duplicated_and_spammers, 0)\n",
    "\n",
    "        X[\"NULL_IN_DATE_TIME\"] = np.where(X[\"DATE\"].isna(), 2, 0)\n",
    "                \n",
    "        result = []\n",
    "        for index, row in X.iterrows():\n",
    "            suspicious_counter = 0\n",
    "            for word in self.suspicious_words_list:\n",
    "                my_row = row[\"CONTENT\"].lower().split()\n",
    "                if word in my_row:\n",
    "                    suspicious_counter += self.spam_dict[word.lower()] // 100\n",
    "            result.append(suspicious_counter)\n",
    "\n",
    "        X[\"SUSPICIOUS_WORDS_COUNT\"] = result\n",
    "\n",
    "        return X\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_suspicious_words(X, y, num):\n",
    "        result = {}\n",
    "        for index, row in X.iterrows():\n",
    "            if y[index] != num:\n",
    "                continue\n",
    "            words = (row[\"CONTENT\"].lower()).split()\n",
    "            for word in words:\n",
    "                if word in result:\n",
    "                    result[word] += 1\n",
    "                else:\n",
    "                    result[word] = 1\n",
    "                    \n",
    "        return result\n",
    "\n",
    "# DEMO\n",
    "transformed = ExplorativeTransformer().fit_transform(train_X, train_y)\n",
    "transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>INTERPRET</th>\n",
       "      <th>DATE_MISSING</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1865</th>\n",
       "      <td>_2viQ_Qnc6_FlLJN0izQaKVQNe6LGDmPZMmkVDjjymE</td>\n",
       "      <td>Neeru bala</td>\n",
       "      <td>2013-09-05 23:07:09.056</td>\n",
       "      <td>Hi.. Everyone.. If anyone after real online wo...</td>\n",
       "      <td>shakira</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>LneaDw26bFsltJodWnZAafXscqrATBuKDM8-8lA4TQE</td>\n",
       "      <td>miamiscraziest</td>\n",
       "      <td>NaT</td>\n",
       "      <td>LADIES!!! -----&amp;gt;&amp;gt; If you have a broken h...</td>\n",
       "      <td>eminem</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       COMMENT_ID          AUTHOR  \\\n",
       "1865  _2viQ_Qnc6_FlLJN0izQaKVQNe6LGDmPZMmkVDjjymE      Neeru bala   \n",
       "1427  LneaDw26bFsltJodWnZAafXscqrATBuKDM8-8lA4TQE  miamiscraziest   \n",
       "\n",
       "                        DATE  \\\n",
       "1865 2013-09-05 23:07:09.056   \n",
       "1427                     NaT   \n",
       "\n",
       "                                                CONTENT INTERPRET  \\\n",
       "1865  Hi.. Everyone.. If anyone after real online wo...   shakira   \n",
       "1427  LADIES!!! -----&gt;&gt; If you have a broken h...    eminem   \n",
       "\n",
       "      DATE_MISSING  \n",
       "1865         False  \n",
       "1427          True  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AddMissingDateColumn(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Adds boolean column if DATE value is missing.\n",
    "    The model should then understand that True most likely means spam.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # do not modify the original dataset\n",
    "        X = X.copy()\n",
    "        X[\"DATE_MISSING\"] = X.DATE.isna()\n",
    "        return X\n",
    "    \n",
    "# DEMO\n",
    "transformed = AddMissingDateColumn().transform(train_X)\n",
    "transformed[1244:1246]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>INTERPRET</th>\n",
       "      <th>LONG_COMMENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>z13wxtdpeznid12et23ogtd4zoyvzbnoz04</td>\n",
       "      <td>Sonny Carter</td>\n",
       "      <td>2015-05-22 11:46:35.988</td>\n",
       "      <td>I love this song sooooooooooooooo much﻿</td>\n",
       "      <td>eminem</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1846</th>\n",
       "      <td>_2viQ_Qnc6-adCzTDLAhqNVQ5hFYcjPyPI5m7pHY4BY</td>\n",
       "      <td>Lizzy Molly</td>\n",
       "      <td>2013-09-09 17:34:07.052</td>\n",
       "      <td>PLEASE CHECK OUT MY VIDEO CALLED &amp;quot;WE LOVE...</td>\n",
       "      <td>shakira</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       COMMENT_ID        AUTHOR  \\\n",
       "1447          z13wxtdpeznid12et23ogtd4zoyvzbnoz04  Sonny Carter   \n",
       "1846  _2viQ_Qnc6-adCzTDLAhqNVQ5hFYcjPyPI5m7pHY4BY   Lizzy Molly   \n",
       "\n",
       "                        DATE  \\\n",
       "1447 2015-05-22 11:46:35.988   \n",
       "1846 2013-09-09 17:34:07.052   \n",
       "\n",
       "                                                CONTENT INTERPRET  \\\n",
       "1447            I love this song sooooooooooooooo much﻿    eminem   \n",
       "1846  PLEASE CHECK OUT MY VIDEO CALLED &quot;WE LOVE...   shakira   \n",
       "\n",
       "      LONG_COMMENT  \n",
       "1447         False  \n",
       "1846          True  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AddLongCommentColumn(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Adds boolean column if CONTENT is longer or equal to 50 charcters.\n",
    "    The model should then understand that True most likely means spam.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        X[\"LONG_COMMENT\"] = X.CONTENT.str.len() >= 50\n",
    "        return X\n",
    "\n",
    "# DEMO\n",
    "transformed = AddLongCommentColumn().transform(train_X)\n",
    "transformed[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>INTERPRET</th>\n",
       "      <th>CONTAINS_CHECK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>z13uutbriumnuj3rq04ccbvqlwjuj1srhyk0k</td>\n",
       "      <td>Warcorpse666</td>\n",
       "      <td>2015-05-26 02:27:43.254</td>\n",
       "      <td>sorry but eminmem is a worthless wife beating ...</td>\n",
       "      <td>eminem</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>z121gbuy2unhc5m4n04cf3kyslqhepeqgvo0k</td>\n",
       "      <td>Santeri Saariokari</td>\n",
       "      <td>2014-09-03 16:32:59.000</td>\n",
       "      <td>Hey guys go to check my video name \"growtopia ...</td>\n",
       "      <td>katy</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 COMMENT_ID              AUTHOR  \\\n",
       "1304  z13uutbriumnuj3rq04ccbvqlwjuj1srhyk0k        Warcorpse666   \n",
       "402   z121gbuy2unhc5m4n04cf3kyslqhepeqgvo0k  Santeri Saariokari   \n",
       "\n",
       "                        DATE  \\\n",
       "1304 2015-05-26 02:27:43.254   \n",
       "402  2014-09-03 16:32:59.000   \n",
       "\n",
       "                                                CONTENT INTERPRET  \\\n",
       "1304  sorry but eminmem is a worthless wife beating ...    eminem   \n",
       "402   Hey guys go to check my video name \"growtopia ...      katy   \n",
       "\n",
       "      CONTAINS_CHECK  \n",
       "1304           False  \n",
       "402             True  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AddContainsCheckColumn(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Adds boolean column if CONTENT contains 'check'.\n",
    "    The model should then understand that True most likely means spam.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        X[\"CONTAINS_CHECK\"] = X.CONTENT.str.contains(\"check\")\n",
    "        return X\n",
    "\n",
    "# DEMO\n",
    "transformed = AddContainsCheckColumn().transform(train_X)\n",
    "transformed[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>INTERPRET</th>\n",
       "      <th>MULTIPLE_COMMENTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1846</th>\n",
       "      <td>_2viQ_Qnc6-adCzTDLAhqNVQ5hFYcjPyPI5m7pHY4BY</td>\n",
       "      <td>Lizzy Molly</td>\n",
       "      <td>2013-09-09 17:34:07.052</td>\n",
       "      <td>PLEASE CHECK OUT MY VIDEO CALLED &amp;quot;WE LOVE...</td>\n",
       "      <td>shakira</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       COMMENT_ID       AUTHOR  \\\n",
       "1846  _2viQ_Qnc6-adCzTDLAhqNVQ5hFYcjPyPI5m7pHY4BY  Lizzy Molly   \n",
       "\n",
       "                        DATE  \\\n",
       "1846 2013-09-09 17:34:07.052   \n",
       "\n",
       "                                                CONTENT INTERPRET  \\\n",
       "1846  PLEASE CHECK OUT MY VIDEO CALLED &quot;WE LOVE...   shakira   \n",
       "\n",
       "      MULTIPLE_COMMENTS  \n",
       "1846              False  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AddMultipleCommentsColumn(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Adds boolean column if author posted multiple comments.\n",
    "    The model should then understand that True most likely means spam.\n",
    "    \n",
    "    Looks also into data it has been fit on.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.prev_X = X.copy()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        joined = pd.concat([X, self.prev_X])\n",
    "        joined.drop_duplicates(inplace=True)\n",
    "        X[\"MULTIPLE_COMMENTS\"] = joined.duplicated(subset=[\"AUTHOR\"], keep=False)[:len(X)]\n",
    "        return X\n",
    "    \n",
    "# DEMO\n",
    "mollys = train_X[train_X.AUTHOR == \"Lizzy Molly\"]\n",
    "admc = AddMultipleCommentsColumn()\n",
    "admc.fit_transform(mollys[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>INTERPRET</th>\n",
       "      <th>MULTIPLE_COMMENTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1472</th>\n",
       "      <td>LneaDw26bFuhuiZ8uX6C-qYLIsOFj9BIWtKWtCz870c</td>\n",
       "      <td>Lizzy Molly</td>\n",
       "      <td>NaT</td>\n",
       "      <td>PLEASE CHECK OUT MY VIDEO CALLED &amp;quot;WE LOVE...</td>\n",
       "      <td>eminem</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       COMMENT_ID       AUTHOR DATE  \\\n",
       "1472  LneaDw26bFuhuiZ8uX6C-qYLIsOFj9BIWtKWtCz870c  Lizzy Molly  NaT   \n",
       "\n",
       "                                                CONTENT INTERPRET  \\\n",
       "1472  PLEASE CHECK OUT MY VIDEO CALLED &quot;WE LOVE...    eminem   \n",
       "\n",
       "      MULTIPLE_COMMENTS  \n",
       "1472               True  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed = admc.transform(mollys[1:2])\n",
    "transformed[transformed.AUTHOR == \"Lizzy Molly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>INTERPRET</th>\n",
       "      <th>MULTIPLE_COMMENTS_SAME_VIDEO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1846</th>\n",
       "      <td>_2viQ_Qnc6-adCzTDLAhqNVQ5hFYcjPyPI5m7pHY4BY</td>\n",
       "      <td>Lizzy Molly</td>\n",
       "      <td>2013-09-09 17:34:07.052</td>\n",
       "      <td>PLEASE CHECK OUT MY VIDEO CALLED &amp;quot;WE LOVE...</td>\n",
       "      <td>shakira</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472</th>\n",
       "      <td>LneaDw26bFuhuiZ8uX6C-qYLIsOFj9BIWtKWtCz870c</td>\n",
       "      <td>Lizzy Molly</td>\n",
       "      <td>NaT</td>\n",
       "      <td>PLEASE CHECK OUT MY VIDEO CALLED &amp;quot;WE LOVE...</td>\n",
       "      <td>eminem</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       COMMENT_ID       AUTHOR  \\\n",
       "1846  _2viQ_Qnc6-adCzTDLAhqNVQ5hFYcjPyPI5m7pHY4BY  Lizzy Molly   \n",
       "1472  LneaDw26bFuhuiZ8uX6C-qYLIsOFj9BIWtKWtCz870c  Lizzy Molly   \n",
       "\n",
       "                        DATE  \\\n",
       "1846 2013-09-09 17:34:07.052   \n",
       "1472                     NaT   \n",
       "\n",
       "                                                CONTENT INTERPRET  \\\n",
       "1846  PLEASE CHECK OUT MY VIDEO CALLED &quot;WE LOVE...   shakira   \n",
       "1472  PLEASE CHECK OUT MY VIDEO CALLED &quot;WE LOVE...    eminem   \n",
       "\n",
       "      MULTIPLE_COMMENTS_SAME_VIDEO  \n",
       "1846                         False  \n",
       "1472                         False  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AddMultipleCommentsSameVideoColumn(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Adds boolean column if author posted multiple comments for the same video.\n",
    "    The model should then understand that True most likely means spam.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        X[\"MULTIPLE_COMMENTS_SAME_VIDEO\"] = X.duplicated(subset=[\"AUTHOR\", \"INTERPRET\"], keep=False)\n",
    "        return X\n",
    "    \n",
    "# DEMO    \n",
    "transformed = AddMultipleCommentsSameVideoColumn().transform(train_X)\n",
    "transformed[transformed.AUTHOR == \"Lizzy Molly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>INTERPRET</th>\n",
       "      <th>CONTAINS_HTTP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>z121gbuy2unhc5m4n04cf3kyslqhepeqgvo0k</td>\n",
       "      <td>Santeri Saariokari</td>\n",
       "      <td>2014-09-03 16:32:59</td>\n",
       "      <td>Hey guys go to check my video name \"growtopia ...</td>\n",
       "      <td>katy</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>z133hdqrqpukup0lp22chhoaztrhvxov5</td>\n",
       "      <td>Quinho Divulgaçoes</td>\n",
       "      <td>2014-11-06 19:50:16</td>\n",
       "      <td>me segue ha  https://www.facebook.com/marcos.s...</td>\n",
       "      <td>katy</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                COMMENT_ID              AUTHOR  \\\n",
       "402  z121gbuy2unhc5m4n04cf3kyslqhepeqgvo0k  Santeri Saariokari   \n",
       "652      z133hdqrqpukup0lp22chhoaztrhvxov5  Quinho Divulgaçoes   \n",
       "\n",
       "                   DATE                                            CONTENT  \\\n",
       "402 2014-09-03 16:32:59  Hey guys go to check my video name \"growtopia ...   \n",
       "652 2014-11-06 19:50:16  me segue ha  https://www.facebook.com/marcos.s...   \n",
       "\n",
       "    INTERPRET  CONTAINS_HTTP  \n",
       "402      katy          False  \n",
       "652      katy           True  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AddContainsHttpColumn(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Adds boolean column if CONTENT contains a link.\n",
    "    The model should then understand that True most likely means spam.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        X[\"CONTAINS_HTTP\"] = X.CONTENT.str.contains(\"http\")\n",
    "        return X\n",
    "    \n",
    "# DEMO    \n",
    "transformed = AddContainsHttpColumn().transform(train_X)\n",
    "transformed[3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>INTERPRET</th>\n",
       "      <th>TIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>z13wxtdpeznid12et23ogtd4zoyvzbnoz04</td>\n",
       "      <td>Sonny Carter</td>\n",
       "      <td>2015-05-22 11:46:35.988</td>\n",
       "      <td>I love this song sooooooooooooooo much﻿</td>\n",
       "      <td>eminem</td>\n",
       "      <td>11:46:35.988000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               COMMENT_ID        AUTHOR  \\\n",
       "1447  z13wxtdpeznid12et23ogtd4zoyvzbnoz04  Sonny Carter   \n",
       "\n",
       "                        DATE                                  CONTENT  \\\n",
       "1447 2015-05-22 11:46:35.988  I love this song sooooooooooooooo much﻿   \n",
       "\n",
       "     INTERPRET             TIME  \n",
       "1447    eminem  11:46:35.988000  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AddTimeColumn(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Adds time column.\n",
    "    Hypothesis: spams are posted at night, or, on the contrary, spams are posted during main working hours. Let the model decide...\n",
    "    This probably won't work, because the dates are most likely relative to the time zone, where they were collected.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        X[\"TIME\"] = X.DATE.dt.time\n",
    "        return X\n",
    "    \n",
    "# DEMO    \n",
    "transformed = AddTimeColumn().transform(train_X)\n",
    "transformed[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:  <a href=\"http://www.youtube.com/watch?v=KQ6zr6kCPj8&amp;t=2m19s\">2:19</a> best part﻿\n",
      "After:  <a href=\"http://www.youtube.com/watch?v=KQ6zr6kCPj8&t=2m19s\">2:19</a> best part﻿\n"
     ]
    }
   ],
   "source": [
    "class HtmlUnescaper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    For example, `&amp;` is escaped ampersand. Unescape it and other characters as well.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        X[\"CONTENT\"] = X[\"CONTENT\"].apply(html.unescape)\n",
    "        return X\n",
    "    \n",
    "# DEMO    \n",
    "print(\"Before: \", train_X.CONTENT[700])\n",
    "transformed = HtmlUnescaper().transform(train_X)\n",
    "print(\"After: \", transformed.CONTENT[700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOMRemover(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Remove Byte Order Mark from comments.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        X[\"CONTENT\"] = X[\"CONTENT\"].str.replace(\"\\ufeff\", \"\", regex=False)\n",
    "        return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Before: <a href=\"http://www.youtube.com/watch?v=KQ6zr6kCPj8&amp;t=2m19s\">2:19</a> best part\\ufeff'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEMO\n",
    "\"Before: \" + train_X.CONTENT.loc[700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After: <a href=\"http://www.youtube.com/watch?v=KQ6zr6kCPj8&amp;t=2m19s\">2:19</a> best part'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed = BOMRemover().transform(train_X)\n",
    "\"After: \" + transformed.CONTENT.loc[700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: <a href=\"http://www.youtube.com/watch?v=KQ6zr6kCPj8&amp;t=2m19s\">2:19</a> best part﻿\n",
      "After: anchortag best part﻿\n"
     ]
    }
   ],
   "source": [
    "class AnchorTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transforms all anchor tags into one keyword. \n",
    "    The model will figure out, that the presence of this keyword probably means spam.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        X[\"CONTENT\"] = X[\"CONTENT\"].str.replace(\"(<a.+>)\", \"anchortag\", regex=True)\n",
    "        return X\n",
    "    \n",
    "# DEMO\n",
    "print(\"Before: \" + train_X.CONTENT.loc[700])\n",
    "transformed = AnchorTransformer().transform(train_X)\n",
    "print(\"After: \" + transformed.CONTENT.loc[700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: <a href=\"http://www.youtube.com/watch?v=KQ6zr6kCPj8&amp;t=2m19s\">2:19</a> best part﻿\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/petr.janik/opt/anaconda3/envs/ib031/lib/python3.8/site-packages/pandas/core/strings/accessor.py:101: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  return func(self, *args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "COMMENT_ID             z13uwn2heqndtr5g304ccv5j5kqqzxjadmc0k\n",
       "AUTHOR                                          Corey Wilson\n",
       "DATE                              2015-05-28 21:39:52.376000\n",
       "CONTENT                                           best part﻿\n",
       "INTERPRET                                              lmfao\n",
       "CONTAINS_ANCHOR_TAG                                     True\n",
       "Name: 700, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AddContainsAnchorTagColumn(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Adds new column CONTAINS_ANCHOR_TAG which is True when CONTENT contains <a> tag.\n",
    "    The model should then understand that True most likely means spam.\n",
    "    Removes the link from CONTENT as well.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        X[\"CONTAINS_ANCHOR_TAG\"] = X[\"CONTENT\"].str.contains(\"(<a.+>)\")\n",
    "        X[\"CONTENT\"] = X[\"CONTENT\"].str.replace(\"(<a.+>)\", \"\", regex=True)\n",
    "        return X\n",
    "    \n",
    "# DEMO\n",
    "print(\"Before: \" + train_X.CONTENT.loc[700])\n",
    "transformed = AddContainsAnchorTagColumn().transform(train_X)\n",
    "transformed.loc[700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: People, here is a new network like FB...\n",
      "After: people, here is a new network like fb...\n"
     ]
    }
   ],
   "source": [
    "class Lower(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Makes CONTENT lowercase.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        X[\"CONTENT\"] = X[\"CONTENT\"].str.lower()\n",
    "        return X\n",
    "    \n",
    "# DEMO\n",
    "print(\"Before: \" + train_X.CONTENT.loc[192][:40])\n",
    "transformed = Lower().transform(train_X)\n",
    "print(\"After: \" + transformed.CONTENT.loc[192][:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: subscribe to my channel  /watch?v=NxK32i0HkDs\n",
      "Before: please like :D https://premium.easypromosapp.com/voteme/19924/616375350﻿\n",
      "After: subscribe to my channel  urllink\n",
      "After: please like :D urllink\n"
     ]
    }
   ],
   "source": [
    "class UrlTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transforms all urls into one keyword. \n",
    "    The model will figure out, that the presence of this keyword probably means spam.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        X[\"CONTENT\"] = X[\"CONTENT\"].str.replace(r\"\\S*\\.com\\S*|\\S*watch\\?\\S*\", \"urllink\", regex=True)\n",
    "        return X\n",
    "    \n",
    "# DEMO\n",
    "print(\"Before: \" + train_X.CONTENT.loc[1573])\n",
    "print(\"Before: \" + train_X.CONTENT.loc[14])\n",
    "transformed = UrlTransformer().transform(train_X)\n",
    "print(\"After: \" + transformed.CONTENT.loc[1573])\n",
    "print(\"After: \" + transformed.CONTENT.loc[14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not need to transform emojis, some word embeddings handle them well:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">Will download <b>1.6 GB</b> large word embeddings model.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5047033597406592\n"
     ]
    }
   ],
   "source": [
    "vectors = Magnitude(MagnitudeUtils.download_model('fasttext/medium/wiki-news-300d-1M'))\n",
    "print(vectors.similarity(\":)\", \":D\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 0.0, 0.0, ..., Timestamp('2015-05-22 11:46:35.988000'),\n",
       "        'I love this song sooooooooooooooo much\\ufeff', 'eminem']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot-encode AUTHOR column\n",
    "pipeline = make_pipeline(\n",
    "    make_column_transformer(\n",
    "        (OneHotEncoder(), [\"AUTHOR\"]),\n",
    "        remainder=\"passthrough\",\n",
    "        sparse_threshold=0\n",
    "    )\n",
    ")\n",
    "# DEMO\n",
    "pipeline.fit(train_X, train_y)\n",
    "pipeline.transform(train_X[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible models and techniques to use:\n",
    "- [Bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model)\n",
    "- [Naive Bayes](https://www.kaggle.com/mohammedakhil/youtube-spam-filter-using-naive-bayes)\n",
    "- [LSTM](https://keras.io/api/layers/recurrent_layers/lstm/)\n",
    "- [DecisionTreeClassifier](https://datascience.stackexchange.com/questions/67250/decision-tree-in-sentiment-analysis)\n",
    "- [Magnitude Word Embeddings](https://colab.research.google.com/drive/1lOcAhIffLW8XC6QsKzt5T_ZqPP4Y9eS4#scrollTo=95Xg9EyU-ZYr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = load_train_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completely random: 0.50\n",
      "proportional: 0.48\n",
      "most frequent: 0.48\n"
     ]
    }
   ],
   "source": [
    "for strategy, strategy_name in zip([\"uniform\", \"stratified\", \"most_frequent\"],\n",
    "                                   [\"completely random\", \"proportional\", \"most frequent\"]):\n",
    "    dummy_clf = DummyClassifier(strategy=strategy)\n",
    "    dummy_clf.fit(train_X, train_y)\n",
    "    print(f\"{strategy_name}: {dummy_clf.score(test_X, test_y):.2f}\") # ignores the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words + Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count vect.; ComplementNB: 0.95\n",
      "Count vect.; ComplementNB - FP: 8, FN: 8\n",
      "Count vect.; BernoulliNB: 0.89\n",
      "Count vect.; BernoulliNB - FP: 4, FN: 29\n",
      "Count vect.; MultinomialNB: 0.94\n",
      "Count vect.; MultinomialNB - FP: 10, FN: 8\n",
      "TF-IDF vect.; ComplementNB: 0.94\n",
      "TF-IDF vect.; ComplementNB - FP: 8, FN: 10\n",
      "TF-IDF vect.; BernoulliNB: 0.89\n",
      "TF-IDF vect.; BernoulliNB - FP: 4, FN: 29\n",
      "TF-IDF vect.; MultinomialNB: 0.94\n",
      "TF-IDF vect.; MultinomialNB - FP: 10, FN: 10\n",
      "Count binary vect.; ComplementNB: 0.95\n",
      "Count binary vect.; ComplementNB - FP: 8, FN: 9\n",
      "Count binary vect.; BernoulliNB: 0.89\n",
      "Count binary vect.; BernoulliNB - FP: 4, FN: 29\n",
      "Count binary vect.; MultinomialNB: 0.94\n",
      "Count binary vect.; MultinomialNB - FP: 10, FN: 9\n",
      "Hashing vect.; ComplementNB: 0.91\n",
      "Hashing vect.; ComplementNB - FP: 21, FN: 7\n",
      "Hashing vect.; BernoulliNB: 0.91\n",
      "Hashing vect.; BernoulliNB - FP: 6, FN: 23\n",
      "Hashing vect.; MultinomialNB: 0.90\n",
      "Hashing vect.; MultinomialNB - FP: 24, FN: 7\n"
     ]
    }
   ],
   "source": [
    "N_FEATURES = len(CountVectorizer().fit(train_X).vocabulary_)*10\n",
    "\n",
    "vectorizers = [(\"Count vect.\", CountVectorizer()), \n",
    "               (\"TF-IDF vect.\", TfidfVectorizer()), \n",
    "               (\"Count binary vect.\", CountVectorizer(binary=True)),\n",
    "               (\"Hashing vect.\", HashingVectorizer(n_features = N_FEATURES, alternate_sign=False)),\n",
    "              ]\n",
    "bayeses = [\n",
    "    (\"ComplementNB\", ComplementNB()), \n",
    "    (\"BernoulliNB\", BernoulliNB()), \n",
    "    (\"MultinomialNB\", MultinomialNB())\n",
    "]\n",
    "\n",
    "for vectorizer_name, vectorizer in vectorizers:\n",
    "    for bayes_name, bayes in bayeses:\n",
    "        pipeline= make_pipeline(\n",
    "            vectorizer,\n",
    "            bayes\n",
    "        )\n",
    "\n",
    "        pipeline.fit(np.array(train_X), train_y)\n",
    "        y_pred = pipeline.predict(test_X)\n",
    "\n",
    "        print(f\"{vectorizer_name}; {bayes_name}: {pipeline.score(test_X, test_y):.2f}\")\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(test_y, y_pred).ravel()\n",
    "        print(f\"{vectorizer_name}; {bayes_name} - FP: {fp}, FN: {fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings + Keras LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 30 # he maximum number of words the sequence model will consider\n",
    "STD_DEV = 0.01 # Deviation of noise for Gaussian Noise applied to the embeddings\n",
    "HIDDEN_UNITS = 100 # The number of hidden units from the LSTM\n",
    "DROPOUT_RATIO = .8 # The ratio to dropout\n",
    "BATCH_SIZE = 100 # The number of examples per train/validation step\n",
    "EPOCHS = 50 # The number of times to repeat through all of the training data\n",
    "LEARNING_RATE = .01 # The learning rate for the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">Will download <b>1.6 GB</b> large word embeddings model.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = Magnitude(MagnitudeUtils.download_model('fasttext/medium/wiki-news-300d-1M'), pad_to_length = MAX_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = load_train_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 30, 300)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WordEmbeddings(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    ! Works with Series, not DataFrame !\n",
    "    For each row in series, which contains a sentence, \n",
    "    it embeds the words in the series into 300 dimensional vectors (one vector for each word).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        tokenized = [word_tokenize(line) for line in X]\n",
    "        return vectors.query(tokenized)\n",
    "    \n",
    "# DEMO\n",
    "transformed = WordEmbeddings().transform(train_X[:10])\n",
    "transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GaussianNoise** - useful to mitigate overfitting (could be used as a form of random data augmentation). Gaussian Noise (GS) is a natural choice as corruption process for real valued inputs.\n",
    "As it is a regularization layer, it is only active at training time.\n",
    "\n",
    "**Bidirectional** – sequence processing model that consists of two LSTMs: one taking the input in a forward direction, and the other in a backwards direction. Bidirectional LSTMs effectively increase the amount of information available to the network, improving the context available to the algorithm\n",
    "\n",
    "**Dropout** – a stochastic regularization technique and should reduce overfitting by (theoretically) combining many different neural network architectures.\n",
    "With Dropout, the training process essentially drops out neurons in a neural network.\n",
    "\n",
    "**Dense** - The dense layer is a neural network layer that is connected deeply, which means each neuron in the dense layer receives input from all neurons of its previous layer.\n",
    "In the background, the dense layer performs a matrix-vector multiplication. The values used in the matrix are actually parameters that can be trained and updated with the help of backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(GaussianNoise(STD_DEV, input_shape=(MAX_WORDS, vectors.dim)))\n",
    "    model.add(Bidirectional(LSTM(HIDDEN_UNITS, activation='tanh'), merge_mode='concat'))\n",
    "    model.add(Dropout(DROPOUT_RATIO))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=Adam(lr=LEARNING_RATE),\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "36/36 [==============================] - 4s 59ms/step - loss: 0.5529 - accuracy: 0.7306 - val_loss: 0.3693 - val_accuracy: 0.8400\n",
      "Epoch 2/5\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.2797 - accuracy: 0.8880 - val_loss: 0.2449 - val_accuracy: 0.9440\n",
      "Epoch 3/5\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.2379 - accuracy: 0.9059 - val_loss: 0.1580 - val_accuracy: 0.9360\n",
      "Epoch 4/5\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 0.1861 - accuracy: 0.9270 - val_loss: 0.1521 - val_accuracy: 0.9440\n",
      "Epoch 5/5\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.1702 - accuracy: 0.9379 - val_loss: 0.1722 - val_accuracy: 0.9280\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.1874 - accuracy: 0.9201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.920127809047699"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = make_pipeline(\n",
    "    WordEmbeddings(),\n",
    "    KerasClassifier(create_model, epochs=5, batch_size=32, validation_split=0.1)\n",
    ")\n",
    "\n",
    "clf.fit(train_X, train_y)\n",
    "clf.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/petr.janik/opt/anaconda3/envs/ib031/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[149  14]\n",
      " [ 11 139]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ham       0.93      0.91      0.92       163\n",
      "        Spam       0.91      0.93      0.92       150\n",
      "\n",
      "    accuracy                           0.92       313\n",
      "   macro avg       0.92      0.92      0.92       313\n",
      "weighted avg       0.92      0.92      0.92       313\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_pred_y = (clf.predict(test_X) > 0.5).astype(\"int32\")\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(test_y, lstm_pred_y))\n",
    "print(\"\\nClassification Report\")\n",
    "print(classification_report(test_y, lstm_pred_y, target_names=[\"Ham\", \"Spam\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = load_train_test_all_cols_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC accuracy score :0.8785942492012779\n"
     ]
    }
   ],
   "source": [
    "svc = make_pipeline(\n",
    "    ExplorativeTransformer(),\n",
    "    ColumnSelector([\"SUSPICIOUS_WORDS_COUNT\", \"NULL_IN_DATE_TIME\", \"HAS_LINK\", \"NOT_UNIQUE_AUTHOR\"]),\n",
    "    SVC(),\n",
    ")\n",
    "\n",
    "svc.fit(train_X, train_y)\n",
    "svc_pred_y = svc.predict(test_X)\n",
    "print(\"SVC accuracy score :\" + str(accuracy_score(test_y, svc_pred_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[159   4]\n",
      " [ 34 116]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ham       0.82      0.98      0.89       163\n",
      "        Spam       0.97      0.77      0.86       150\n",
      "\n",
      "    accuracy                           0.88       313\n",
      "   macro avg       0.90      0.87      0.88       313\n",
      "weighted avg       0.89      0.88      0.88       313\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(test_y, svc_pred_y))\n",
    "print(\"\\nClassification Report\")\n",
    "print(classification_report(test_y, svc_pred_y, target_names=[\"Ham\", \"Spam\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "* splitting the space by setting rules\n",
    "* removing the unnecessary splits\n",
    "* using the class with majority votes as the prediction\n",
    "\n",
    "Decision tree first splits the data based on these concepts: Pure and Impure, Impurity measurement, Information Gain.\n",
    "This is calculated when traversing trough the tree, then edge with more gain is used to decide about the categorization of an instance\n",
    "given to the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1249, 301)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WordEmbeddingsDF(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    For each column for each row, computes word embedddings of values in the column split with nltk.word_tokenize.\n",
    "    The mean of these word embeddings is then computed, giving 300 dimensional vector for each row.\n",
    "    This vector is then appended to the dataframe as 300 new columns.\n",
    "    \"\"\"\n",
    "    def init(self):\n",
    "        super().init()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        for col in X:\n",
    "            tokenized = [word_tokenize(line) for line in X[col]]\n",
    "            mean = np.mean(vectors.query(tokenized), axis=1)\n",
    "            for i in range(300):\n",
    "                X[f\"{i}_EMBEDDED\"] = mean[:,i]\n",
    "\n",
    "        return X\n",
    "    \n",
    "transformed = WordEmbeddingsDF().transform(train_X[[\"CONTENT\"]])\n",
    "transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTC accuracy score :0.9648562300319489\n"
     ]
    }
   ],
   "source": [
    "dtc = make_pipeline(\n",
    "    CountVectorizer(stop_words=\"english\"),\n",
    "    DecisionTreeClassifier()\n",
    ")\n",
    "\n",
    "dtc.fit(train_X[\"CONTENT\"], train_y)\n",
    "print(\"DTC accuracy score :\" + str(dtc.score(test_X[\"CONTENT\"], test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[159   4]\n",
      " [  7 143]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ham       0.96      0.98      0.97       163\n",
      "        Spam       0.97      0.95      0.96       150\n",
      "\n",
      "    accuracy                           0.96       313\n",
      "   macro avg       0.97      0.96      0.96       313\n",
      "weighted avg       0.96      0.96      0.96       313\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtc_pred_y = dtc.predict(test_X[\"CONTENT\"])\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(test_y, dtc_pred_y))\n",
    "print(\"\\nClassification Report\")\n",
    "print(classification_report(test_y, dtc_pred_y, target_names=[\"Ham\", \"Spam\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "**Baseline MNBayes** - 0.9–0.95\n",
    "\n",
    "**LSTM** - 0.93\n",
    "\n",
    "This model uses Magnitude library to convert words into vectors of numbers (word embeddings). This conversion, combined with LSTM, preserves the order of words.\n",
    "However, MLP with CountVectorizer, which does not preserve the order performed slightly better (0.95). This iundicates that the architecture and hyperparameters of the used LSTM still need some fine tuning.\n",
    "\n",
    "**SVC** - 0.87\n",
    "\n",
    "This result was reached with our state-of-the-art ExplorationTransformer. The SVM classifier was also tried with a Count Vectorizer and, interestingly, it performed better with stop words included than without them (0.94 vs 0.92).\n",
    "\n",
    "**Tree Classifier** - 0.95\n",
    "\n",
    "Considering the relative simplicity of this model, it performed rather well on the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have prepared many transformers, which could be used to create new columns. \n",
    "We could then try different combinations of these columns and select the best, perhaps by using SelectKBest.\n",
    "\n",
    "However, considering the success of even the baseline model, and the success of other models on just the vectorized comments, the performance on this\n",
    "dataset might increase only marginally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ib031] *",
   "language": "python",
   "name": "conda-env-ib031-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
