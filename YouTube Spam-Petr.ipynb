{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Detection in YouTube Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#nlp #classification\n",
    "\n",
    "Your task is to classify YouTube comments into spam and ham categories. You need to use something other than bag of words and Naive Bayes as your models (but it can be a baseline).\n",
    "\n",
    "Dataset source: https://archive.ics.uci.edu/ml/datasets/YouTube+Spam+Collection.\n",
    "\n",
    "Assignment: https://is.muni.cz/auth/el/fi/jaro2021/IB031/um/cviceni/assignment.pdf?predmet=1323750"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:  <a href=\"http://www.youtube.com/watch?v=KQ6zr6kCPj8&amp;t=2m19s\">2:19</a> best part﻿\n",
      "After:  <a href=\"http://www.youtube.com/watch?v=KQ6zr6kCPj8&t=2m19s\">2:19</a> best part﻿\n",
      "Before: <a href=\"http://www.youtube.com/watch?v=KQ6zr6kCPj8&amp;t=2m19s\">2:19</a> best part﻿\n",
      "After: anchortag best part﻿\n",
      "Before: <a href=\"http://www.youtube.com/watch?v=KQ6zr6kCPj8&amp;t=2m19s\">2:19</a> best part﻿\n",
      "Before: People, here is a new network like FB...\n",
      "After: people, here is a new network like fb...\n",
      "Before: subscribe to my channel  /watch?v=NxK32i0HkDs\n",
      "Before: please like :D https://premium.easypromosapp.com/voteme/19924/616375350﻿\n",
      "After: subscribe to my channel  urllink\n",
      "After: please like :D urllink\n",
      "                                       COMMENT_ID              AUTHOR  \\\n",
      "1447          z13wxtdpeznid12et23ogtd4zoyvzbnoz04        Sonny Carter   \n",
      "1846  _2viQ_Qnc6-adCzTDLAhqNVQ5hFYcjPyPI5m7pHY4BY         Lizzy Molly   \n",
      "1304        z13uutbriumnuj3rq04ccbvqlwjuj1srhyk0k        Warcorpse666   \n",
      "402         z121gbuy2unhc5m4n04cf3kyslqhepeqgvo0k  Santeri Saariokari   \n",
      "652             z133hdqrqpukup0lp22chhoaztrhvxov5  Quinho Divulgaçoes   \n",
      "\n",
      "                        DATE  \\\n",
      "1447 2015-05-22 11:46:35.988   \n",
      "1846 2013-09-09 17:34:07.052   \n",
      "1304 2015-05-26 02:27:43.254   \n",
      "402  2014-09-03 16:32:59.000   \n",
      "652  2014-11-06 19:50:16.000   \n",
      "\n",
      "                                                CONTENT INTERPRET  \n",
      "1447            I love this song sooooooooooooooo much﻿    eminem  \n",
      "1846  PLEASE CHECK OUT MY VIDEO CALLED &quot;WE LOVE...   shakira  \n",
      "1304  sorry but eminmem is a worthless wife beating ...    eminem  \n",
      "402   Hey guys go to check my video name \"growtopia ...      katy  \n",
      "652   me segue ha  https://www.facebook.com/marcos.s...      katy  \n"
     ]
    }
   ],
   "source": [
    "# Local\n",
    "from ipynb.fs.full.data_loader import load_train_test_data, load_train_test_all_cols_data\n",
    "from ipynb.fs.full.transformers import (FeatureSelector, WordEmbeddingsSeries, \n",
    "                                        WordEmbeddingsDF, ExplorativeTransformer, Debugger)\n",
    "from ipynb.fs.full.utils import print_report\n",
    "\n",
    "# Math and data stuff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB, ComplementNB, BernoulliNB, MultinomialNB, CategoricalNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Tensorflow Keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GaussianNoise, LSTM, Bidirectional, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Language stuff\n",
    "from pymagnitude import Magnitude, MagnitudeUtils\n",
    "\n",
    "# Other\n",
    "import html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data\"\n",
    "\n",
    "psy = pd.read_csv(f\"{data_dir}/Youtube01-Psy.csv\", parse_dates=[\"DATE\"])\n",
    "katy = pd.read_csv(f\"{data_dir}/Youtube02-KatyPerry.csv\", parse_dates=[\"DATE\"])\n",
    "lmfao = pd.read_csv(f\"{data_dir}/Youtube03-LMFAO.csv\", parse_dates=[\"DATE\"])\n",
    "eminem = pd.read_csv(f\"{data_dir}/Youtube04-Eminem.csv\", parse_dates=[\"DATE\"])\n",
    "shakira = pd.read_csv(f\"{data_dir}/Youtube05-Shakira.csv\", parse_dates=[\"DATE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presentation: https://docs.google.com/presentation/d/1GIoRrhQ2_eERuowLotWscQhx7X4s4A4mwBm3-aGpO_E/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset comes from 2017.\n",
    "It has five datasets composed by 1 956 real messages extracted from five videos that were among the 10 most viewed on the collection period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Artist     | Song (our guess)     | Song published on YouTube (date) | Comments range |\n",
    "|------------|----------------------|----------------------------------|----------------|\n",
    "| Psy        | Gangnam Style        | 15. 7. 2012                      | 2013–2015      |\n",
    "| Katy Perry | Roar                 | 5. 9. 2013                       | 2014–2015      |\n",
    "| LMFAO      | Party Rock Anthem    | 9. 3. 2011                       | 2014–2015      |\n",
    "| Eminem     | Love The Way You Lie | 5. 8. 2010                       | 2015-2015      |\n",
    "| Shakira    | Waka Waka            | 5. 6. 2010                       | 2013–2015      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07 06:20:48.000</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A</td>\n",
       "      <td>adam riyati</td>\n",
       "      <td>2013-11-07 12:37:15.000</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8</td>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>2013-11-08 17:34:21.000</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z13jhp0bxqncu512g22wvzkasxmvvzjaz04</td>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>2013-11-09 08:28:43.000</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z13fwbwp1oujthgqj04chlngpvzmtt3r3dw</td>\n",
       "      <td>GsMega</td>\n",
       "      <td>2013-11-10 16:05:38.000</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>_2viQ_Qnc6-bMSjqyL1NKj57ROicCSJV5SwTrw-RFFA</td>\n",
       "      <td>Katie Mettam</td>\n",
       "      <td>2013-07-13 13:27:39.441</td>\n",
       "      <td>I love this song because we sing it at Camp al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>_2viQ_Qnc6-pY-1yR6K2FhmC5i48-WuNx5CumlHLDAI</td>\n",
       "      <td>Sabina Pearson-Smith</td>\n",
       "      <td>2013-07-13 13:14:30.021</td>\n",
       "      <td>I love this song for two reasons: 1.it is abou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>_2viQ_Qnc6_k_n_Bse9zVhJP8tJReZpo8uM2uZfnzDs</td>\n",
       "      <td>jeffrey jules</td>\n",
       "      <td>2013-07-13 12:09:31.188</td>\n",
       "      <td>wow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>_2viQ_Qnc6_yBt8UGMWyg3vh0PulTqcqyQtdE7d4Fl0</td>\n",
       "      <td>Aishlin Maciel</td>\n",
       "      <td>2013-07-13 11:17:52.308</td>\n",
       "      <td>Shakira u are so wiredo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>_2viQ_Qnc685RPw1aSa1tfrIuHXRvAQ2rPT9R06KTqA</td>\n",
       "      <td>Latin Bosch</td>\n",
       "      <td>2013-07-12 22:33:27.916</td>\n",
       "      <td>Shakira is the best dancer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1956 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       COMMENT_ID                AUTHOR  \\\n",
       "0     LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU             Julius NM   \n",
       "1     LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A           adam riyati   \n",
       "2     LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8      Evgeny Murashkin   \n",
       "3             z13jhp0bxqncu512g22wvzkasxmvvzjaz04       ElNino Melendez   \n",
       "4             z13fwbwp1oujthgqj04chlngpvzmtt3r3dw                GsMega   \n",
       "...                                           ...                   ...   \n",
       "1951  _2viQ_Qnc6-bMSjqyL1NKj57ROicCSJV5SwTrw-RFFA          Katie Mettam   \n",
       "1952  _2viQ_Qnc6-pY-1yR6K2FhmC5i48-WuNx5CumlHLDAI  Sabina Pearson-Smith   \n",
       "1953  _2viQ_Qnc6_k_n_Bse9zVhJP8tJReZpo8uM2uZfnzDs         jeffrey jules   \n",
       "1954  _2viQ_Qnc6_yBt8UGMWyg3vh0PulTqcqyQtdE7d4Fl0        Aishlin Maciel   \n",
       "1955  _2viQ_Qnc685RPw1aSa1tfrIuHXRvAQ2rPT9R06KTqA           Latin Bosch   \n",
       "\n",
       "                        DATE  \\\n",
       "0    2013-11-07 06:20:48.000   \n",
       "1    2013-11-07 12:37:15.000   \n",
       "2    2013-11-08 17:34:21.000   \n",
       "3    2013-11-09 08:28:43.000   \n",
       "4    2013-11-10 16:05:38.000   \n",
       "...                      ...   \n",
       "1951 2013-07-13 13:27:39.441   \n",
       "1952 2013-07-13 13:14:30.021   \n",
       "1953 2013-07-13 12:09:31.188   \n",
       "1954 2013-07-13 11:17:52.308   \n",
       "1955 2013-07-12 22:33:27.916   \n",
       "\n",
       "                                                CONTENT  CLASS  \n",
       "0     Huh, anyway check out this you[tube] channel: ...      1  \n",
       "1     Hey guys check out my new channel and our firs...      1  \n",
       "2                just for test I have to say murdev.com      1  \n",
       "3      me shaking my sexy ass on my channel enjoy ^_^ ﻿      1  \n",
       "4               watch?v=vtaRGgvGtWQ   Check this out .﻿      1  \n",
       "...                                                 ...    ...  \n",
       "1951  I love this song because we sing it at Camp al...      0  \n",
       "1952  I love this song for two reasons: 1.it is abou...      0  \n",
       "1953                                                wow      0  \n",
       "1954                            Shakira u are so wiredo      0  \n",
       "1955                         Shakira is the best dancer      0  \n",
       "\n",
       "[1956 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined = pd.concat([psy, katy, lmfao, eminem, shakira]).reset_index(drop=True)\n",
    "joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1956 total rows in the dataset.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(joined)} total rows in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column `CLASS` is the label column we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 1005 spams and 951 hams.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"There are {len(joined[joined.CLASS == 1])} spams and {len(joined[joined.CLASS == 0])} hams.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the data set at the very beginning not to make the test data dirty.\n",
    "`final_test_X` and `final_test_y` will be used at the very end to evaluate and compare our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_X, joined_y = joined.drop(columns=\"CLASS\"), joined.CLASS\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "train_X_orig, final_test_X, train_y, final_test_y = train_test_split(\n",
    "    joined_X, joined_y, test_size=0.2, random_state=random_state\n",
    ")\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    joined, test_size=0.2, random_state=random_state\n",
    ")\n",
    "\n",
    "train_df = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After putting test data apart, there are 1564 train data rows.\n"
     ]
    }
   ],
   "source": [
    "print(f\"After putting test data apart, there are {len(train_X_orig)} train data rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 789 spams and 775 hams.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"There are {(train_y == 1).sum()} spams and {(train_y == 0).sum()} hams.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...which is well balanced, so we do not need any fancy sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1564 entries, 836 to 1126\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   COMMENT_ID  1564 non-null   object        \n",
      " 1   AUTHOR      1564 non-null   object        \n",
      " 2   DATE        1369 non-null   datetime64[ns]\n",
      " 3   CONTENT     1564 non-null   object        \n",
      "dtypes: datetime64[ns](1), object(3)\n",
      "memory usage: 61.1+ KB\n"
     ]
    }
   ],
   "source": [
    "train_X_orig.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset includes the following columns:\n",
    "- COMMENT_ID - string\n",
    "- AUTHOR - string\n",
    "- DATE - date\n",
    "- CONTENT (comment itself) - string\n",
    "- CLASS (1 = spam, 0 = ham) - categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column `COMMENT_ID` provides very little (no) value, let's drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>Blaze Blaziken</td>\n",
       "      <td>2015-05-20 03:50:29.098</td>\n",
       "      <td>you cant stop the shuffle﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1688</th>\n",
       "      <td>Lucia Scarlet</td>\n",
       "      <td>2015-05-22 11:56:53.104</td>\n",
       "      <td>Amazing song﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>Paul Buxton</td>\n",
       "      <td>2015-05-21 11:12:22.066</td>\n",
       "      <td>Omg! This guy sounds like an american professo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1650</th>\n",
       "      <td>Tierra Carbon</td>\n",
       "      <td>2015-05-24 14:21:54.411</td>\n",
       "      <td>It was  cool   the best   song ever  ﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573</th>\n",
       "      <td>David Bottenberg</td>\n",
       "      <td>NaT</td>\n",
       "      <td>subscribe to my channel  /watch?v=NxK32i0HkDs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>ItsJoey Dash</td>\n",
       "      <td>2014-07-22 10:04:05.755</td>\n",
       "      <td>EVERYONE PLEASE SUBSCRIBE TO MY CHANNEL OR CAN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>louis canellony</td>\n",
       "      <td>NaT</td>\n",
       "      <td>watch youtube video &amp;quot;EMINEM -YTMA artist ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>vieshva .d.exodous</td>\n",
       "      <td>2015-05-18 08:38:34.236</td>\n",
       "      <td>Awesome﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>viviane trinh</td>\n",
       "      <td>2015-05-21 22:35:35.753</td>\n",
       "      <td>i like the lyrics but not to music video﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>Juan Martinez</td>\n",
       "      <td>2014-07-22 13:22:54.111</td>\n",
       "      <td>I learned the shuffle because of them﻿</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1564 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  AUTHOR                    DATE  \\\n",
       "836       Blaze Blaziken 2015-05-20 03:50:29.098   \n",
       "1688       Lucia Scarlet 2015-05-22 11:56:53.104   \n",
       "1505         Paul Buxton 2015-05-21 11:12:22.066   \n",
       "1650       Tierra Carbon 2015-05-24 14:21:54.411   \n",
       "1573    David Bottenberg                     NaT   \n",
       "...                  ...                     ...   \n",
       "1130        ItsJoey Dash 2014-07-22 10:04:05.755   \n",
       "1294     louis canellony                     NaT   \n",
       "860   vieshva .d.exodous 2015-05-18 08:38:34.236   \n",
       "1459       viviane trinh 2015-05-21 22:35:35.753   \n",
       "1126       Juan Martinez 2014-07-22 13:22:54.111   \n",
       "\n",
       "                                                CONTENT  \n",
       "836                          you cant stop the shuffle﻿  \n",
       "1688                                      Amazing song﻿  \n",
       "1505  Omg! This guy sounds like an american professo...  \n",
       "1650             It was  cool   the best   song ever  ﻿  \n",
       "1573      subscribe to my channel  /watch?v=NxK32i0HkDs  \n",
       "...                                                 ...  \n",
       "1130  EVERYONE PLEASE SUBSCRIBE TO MY CHANNEL OR CAN...  \n",
       "1294  watch youtube video &quot;EMINEM -YTMA artist ...  \n",
       "860                                            Awesome﻿  \n",
       "1459          i like the lyrics but not to music video﻿  \n",
       "1126             I learned the shuffle because of them﻿  \n",
       "\n",
       "[1564 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = train_X_orig.drop(columns=\"COMMENT_ID\")\n",
    "train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>The `AUTHOR` column can help in classifying spam – spammers may have specific names (e.g. having a \"normal\" name instead of a username, copying a well-known username etc.).</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no correlations between columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis**: Something that looks like a spam is not always a spam, it depends on the video.\n",
    "E.g. if the word appears in the video subtitles, it is less likely to be a spam. However, we will not develop this hypothesis any further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration: Missing values? (Eminem is missing a lot of dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "psy\n",
      "\tCOMMENT_ID: NO missing values\n",
      "\tAUTHOR    : NO missing values\n",
      "\tDATE      : NO missing values\n",
      "\tCONTENT   : NO missing values\n",
      "\tCLASS     : NO missing values\n",
      "katy\n",
      "\tCOMMENT_ID: NO missing values\n",
      "\tAUTHOR    : NO missing values\n",
      "\tDATE      : NO missing values\n",
      "\tCONTENT   : NO missing values\n",
      "\tCLASS     : NO missing values\n",
      "lmfao\n",
      "\tCOMMENT_ID: NO missing values\n",
      "\tAUTHOR    : NO missing values\n",
      "\tDATE      : NO missing values\n",
      "\tCONTENT   : NO missing values\n",
      "\tCLASS     : NO missing values\n",
      "eminem\n",
      "\tCOMMENT_ID: NO missing values\n",
      "\tAUTHOR    : NO missing values\n",
      "\tDATE      : missing values\n",
      "\tCONTENT   : NO missing values\n",
      "\tCLASS     : NO missing values\n",
      "shakira\n",
      "\tCOMMENT_ID: NO missing values\n",
      "\tAUTHOR    : NO missing values\n",
      "\tDATE      : NO missing values\n",
      "\tCONTENT   : NO missing values\n",
      "\tCLASS     : NO missing values\n"
     ]
    }
   ],
   "source": [
    "for dataset, dataset_name in zip([psy, katy, lmfao, eminem, shakira], [\"psy\", \"katy\", \"lmfao\", \"eminem\", \"shakira\"]):\n",
    "    na = dataset.isna().any()\n",
    "    print(dataset_name)\n",
    "    for missing, column_name in zip(na, na.index):\n",
    "        print(f'\\t{column_name:10}: {\"\" if missing else \"NO \"}missing values')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that only eminem dataset has missing valued and they are in the `DATE` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'245 out of 448 DATE values in eminem dataset are missing.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{eminem.DATE.isna().sum()} out of {len(eminem)} DATE values in eminem dataset are missing.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we wanted to replace `NaN` dates with previous `non NaN` date in a sorted dataset, but..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    245\n",
       "Name: CLASS, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eminem[eminem.DATE.isna()][\"CLASS\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>All Eminem comments that do not have DATE are spams. The information that DATE is missing is therefore valuable.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration: What range are comment dates in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "psy: 2013-2015\n",
      "katy: 2014-2015\n",
      "lmfao: 2014-2015\n",
      "eminem: 2015-2015\n",
      "shakira: 2013-2015\n"
     ]
    }
   ],
   "source": [
    "for dataset, dataset_name in zip([psy, katy, lmfao, eminem, shakira], [\"psy\", \"katy\", \"lmfao\", \"eminem\", \"shakira\"]):\n",
    "    sorted_dataset = dataset[~dataset.DATE.isna()].DATE.sort_values()\n",
    "    earliest = sorted_dataset.iloc[0].year\n",
    "    latest = sorted_dataset.iloc[-1].year\n",
    "    print(f\"{dataset_name}: {earliest}-{latest}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis: Longer comment implies spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6304347826086957"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = train_X.CONTENT.str.len() >= 50\n",
    "accuracy_score(train_y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...better than random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis: Spams contain \"check\" (e.g. \"Check out my channel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5818414322250639"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = train_X.CONTENT.str.contains(\"check\")\n",
    "accuracy_score(train_y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...better than random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration: Are there any duplicate comments based on ID?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1421</th>\n",
       "      <td>LneaDw26bFvPh9xBHNw1btQoyP60ay_WWthtvXCx37s</td>\n",
       "      <td>janez novak</td>\n",
       "      <td>NaT</td>\n",
       "      <td>share and like this page to win a hand signed ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>LneaDw26bFuH6iFsSrjlJLJIX3qD4R8-emuZ-aGUj0o</td>\n",
       "      <td>Amir bassem</td>\n",
       "      <td>NaT</td>\n",
       "      <td>if u love rihanna subscribe me</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>LneaDw26bFvPh9xBHNw1btQoyP60ay_WWthtvXCx37s</td>\n",
       "      <td>janez novak</td>\n",
       "      <td>NaT</td>\n",
       "      <td>share and like this page to win a hand signed ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>_2viQ_Qnc68fX3dYsfYuM-m4ELMJvxOQBmBOFHqGOk0</td>\n",
       "      <td>tyler sleetway</td>\n",
       "      <td>2013-10-05 00:57:25.078</td>\n",
       "      <td>so beutiful</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>_2viQ_Qnc68fX3dYsfYuM-m4ELMJvxOQBmBOFHqGOk0</td>\n",
       "      <td>tyler sleetway</td>\n",
       "      <td>2013-10-05 00:57:25.078</td>\n",
       "      <td>so beutiful</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>LneaDw26bFuH6iFsSrjlJLJIX3qD4R8-emuZ-aGUj0o</td>\n",
       "      <td>Amir bassem</td>\n",
       "      <td>NaT</td>\n",
       "      <td>if u love rihanna subscribe me</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       COMMENT_ID          AUTHOR  \\\n",
       "1421  LneaDw26bFvPh9xBHNw1btQoyP60ay_WWthtvXCx37s     janez novak   \n",
       "1441  LneaDw26bFuH6iFsSrjlJLJIX3qD4R8-emuZ-aGUj0o     Amir bassem   \n",
       "1420  LneaDw26bFvPh9xBHNw1btQoyP60ay_WWthtvXCx37s     janez novak   \n",
       "1797  _2viQ_Qnc68fX3dYsfYuM-m4ELMJvxOQBmBOFHqGOk0  tyler sleetway   \n",
       "1798  _2viQ_Qnc68fX3dYsfYuM-m4ELMJvxOQBmBOFHqGOk0  tyler sleetway   \n",
       "1443  LneaDw26bFuH6iFsSrjlJLJIX3qD4R8-emuZ-aGUj0o     Amir bassem   \n",
       "\n",
       "                        DATE  \\\n",
       "1421                     NaT   \n",
       "1441                     NaT   \n",
       "1420                     NaT   \n",
       "1797 2013-10-05 00:57:25.078   \n",
       "1798 2013-10-05 00:57:25.078   \n",
       "1443                     NaT   \n",
       "\n",
       "                                                CONTENT  CLASS  \n",
       "1421  share and like this page to win a hand signed ...      1  \n",
       "1441                     if u love rihanna subscribe me      1  \n",
       "1420  share and like this page to win a hand signed ...      1  \n",
       "1797                                        so beutiful      0  \n",
       "1798                                        so beutiful      0  \n",
       "1443                     if u love rihanna subscribe me      1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df.COMMENT_ID.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>We want to drop duplicate rows.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We lose only 3 rows by dropping duplicates.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"We lose only {len(train_df) - len(train_df.drop_duplicates())} rows by dropping duplicates.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there any duplicate comments based on Author name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>z13xtdlovm2hzl05d04ccz1pnvqtezdriqc0k</td>\n",
       "      <td>Uroš Slemenjak</td>\n",
       "      <td>2014-11-07 12:08:13.000</td>\n",
       "      <td>People, here is a new network like FB...you re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>z12wj5g52rzbcvprl04cenuj1yyifhxq3hw</td>\n",
       "      <td>LuckyMusiqLive</td>\n",
       "      <td>2014-09-15 17:47:57.000</td>\n",
       "      <td>Katy has the voice of gold. this video really ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>z12udxjwpwurtlwz304ccbrhdtusth4herk0k</td>\n",
       "      <td>PacKmaN</td>\n",
       "      <td>2014-11-05 21:56:39.000</td>\n",
       "      <td>check men out i put allot of effort into my mu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>_2viQ_Qnc6_umVgV0fI-CSScDHuFxNHIVvezCGhajW8</td>\n",
       "      <td>Alain Bruno</td>\n",
       "      <td>2013-10-02 04:01:20.922</td>\n",
       "      <td>Shakira is very beautiful</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131</th>\n",
       "      <td>z12dw3tbbzm2gpzty22gtf1bvviqeha2j</td>\n",
       "      <td>ItsJoey Dash</td>\n",
       "      <td>2014-07-22 10:04:00.700</td>\n",
       "      <td>EVERYONE PLEASE SUBSCRIBE TO MY CHANNEL OR CAN...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>z13jvndqyr2zjzpxo04cij4pdva4yhqzlq40k</td>\n",
       "      <td>D Maw</td>\n",
       "      <td>2015-05-25 05:43:54.048</td>\n",
       "      <td>Remeber when this song was good﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>LneaDw26bFuUfz6WhRf9xiRHsxHm0t4fXYLbcPhB7Lk</td>\n",
       "      <td>Scott Johnson</td>\n",
       "      <td>NaT</td>\n",
       "      <td>You guys should check out this EXTRAORDINARY w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>z132svd4fvq1wntfd221w5szfzezjri2r</td>\n",
       "      <td>Abdullah Fawzi</td>\n",
       "      <td>2015-05-25 06:23:24.405</td>\n",
       "      <td>see this&lt;br /&gt;&lt;a href=\"http://adf.ly\"&gt;http://a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>z13qfn5yusqoslnn222dutdw4yqmhzhej</td>\n",
       "      <td>LiveLikeLien x</td>\n",
       "      <td>2015-05-20 00:57:56.444</td>\n",
       "      <td>It makes me happy instantly, and makes me forg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>z121szzyozr4vpqqc04cdn5g4zjhutdosdw</td>\n",
       "      <td>ItsJoey Dash</td>\n",
       "      <td>2014-07-22 10:04:05.755</td>\n",
       "      <td>EVERYONE PLEASE SUBSCRIBE TO MY CHANNEL OR CAN...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>184 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       COMMENT_ID          AUTHOR  \\\n",
       "192         z13xtdlovm2hzl05d04ccz1pnvqtezdriqc0k  Uroš Slemenjak   \n",
       "462           z12wj5g52rzbcvprl04cenuj1yyifhxq3hw  LuckyMusiqLive   \n",
       "141         z12udxjwpwurtlwz304ccbrhdtusth4herk0k         PacKmaN   \n",
       "1828  _2viQ_Qnc6_umVgV0fI-CSScDHuFxNHIVvezCGhajW8     Alain Bruno   \n",
       "1131            z12dw3tbbzm2gpzty22gtf1bvviqeha2j    ItsJoey Dash   \n",
       "...                                           ...             ...   \n",
       "769         z13jvndqyr2zjzpxo04cij4pdva4yhqzlq40k           D Maw   \n",
       "1332  LneaDw26bFuUfz6WhRf9xiRHsxHm0t4fXYLbcPhB7Lk   Scott Johnson   \n",
       "1638            z132svd4fvq1wntfd221w5szfzezjri2r  Abdullah Fawzi   \n",
       "1724            z13qfn5yusqoslnn222dutdw4yqmhzhej  LiveLikeLien x   \n",
       "1130          z121szzyozr4vpqqc04cdn5g4zjhutdosdw    ItsJoey Dash   \n",
       "\n",
       "                        DATE  \\\n",
       "192  2014-11-07 12:08:13.000   \n",
       "462  2014-09-15 17:47:57.000   \n",
       "141  2014-11-05 21:56:39.000   \n",
       "1828 2013-10-02 04:01:20.922   \n",
       "1131 2014-07-22 10:04:00.700   \n",
       "...                      ...   \n",
       "769  2015-05-25 05:43:54.048   \n",
       "1332                     NaT   \n",
       "1638 2015-05-25 06:23:24.405   \n",
       "1724 2015-05-20 00:57:56.444   \n",
       "1130 2014-07-22 10:04:05.755   \n",
       "\n",
       "                                                CONTENT  CLASS  \n",
       "192   People, here is a new network like FB...you re...      1  \n",
       "462   Katy has the voice of gold. this video really ...      1  \n",
       "141   check men out i put allot of effort into my mu...      1  \n",
       "1828                          Shakira is very beautiful      0  \n",
       "1131  EVERYONE PLEASE SUBSCRIBE TO MY CHANNEL OR CAN...      1  \n",
       "...                                                 ...    ...  \n",
       "769                    Remeber when this song was good﻿      0  \n",
       "1332  You guys should check out this EXTRAORDINARY w...      1  \n",
       "1638  see this<br /><a href=\"http://adf.ly\">http://a...      1  \n",
       "1724  It makes me happy instantly, and makes me forg...      0  \n",
       "1130  EVERYONE PLEASE SUBSCRIBE TO MY CHANNEL OR CAN...      1  \n",
       "\n",
       "[184 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_authors = train_df[train_df.duplicated(subset=[\"AUTHOR\"], keep=False)]\n",
    "duplicate_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>z13xtdlovm2hzl05d04ccz1pnvqtezdriqc0k</td>\n",
       "      <td>Uroš Slemenjak</td>\n",
       "      <td>2014-11-07 12:08:13</td>\n",
       "      <td>People, here is a new network like FB...you re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>z13ocdbaxwqdvjnwx04ccz1pnvqtezdriqc0k</td>\n",
       "      <td>Uroš Slemenjak</td>\n",
       "      <td>2014-11-07 12:16:21</td>\n",
       "      <td>People, here is a new network like FB...you re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                COMMENT_ID          AUTHOR  \\\n",
       "192  z13xtdlovm2hzl05d04ccz1pnvqtezdriqc0k  Uroš Slemenjak   \n",
       "658  z13ocdbaxwqdvjnwx04ccz1pnvqtezdriqc0k  Uroš Slemenjak   \n",
       "\n",
       "                   DATE                                            CONTENT  \\\n",
       "192 2014-11-07 12:08:13  People, here is a new network like FB...you re...   \n",
       "658 2014-11-07 12:16:21  People, here is a new network like FB...you re...   \n",
       "\n",
       "     CLASS  \n",
       "192      1  \n",
       "658      1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df.AUTHOR == \"Uroš Slemenjak\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    149\n",
       "0     35\n",
       "Name: CLASS, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_authors[\"CLASS\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'80.98% of comments from authors who posted multiple comments are spams.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spams, hams = duplicate_authors[\"CLASS\"].value_counts()\n",
    "f\"{(spams / (spams + hams)) * 100:.02f}% of comments from authors who posted multiple comments are spams.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the same author post both spams and hams? If so, are the labels truly correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 0 authors who posted both spam and ham. But this might not be generally '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_by_author = duplicate_authors.groupby(\"AUTHOR\").CLASS.mean()\n",
    "posted_both_spam_and_ham = (grouped_by_author != 0) & (grouped_by_author != 1)\n",
    "f\"There are {len(grouped_by_author[posted_both_spam_and_ham])} authors who posted both spam and ham. But this might not be generally \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 0 comments that are classified both as spam and ham.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_comments = train_df[train_df.duplicated(subset=[\"CONTENT\"], keep=False)]\n",
    "grouped_by_content = duplicate_comments.groupby(\"CONTENT\").CLASS.mean()\n",
    "same_content_spam_and_ham = (grouped_by_content != 0) & (grouped_by_content != 1)\n",
    "f\"There are {len(grouped_by_content[same_content_spam_and_ham])} comments that are classified both as spam and ham.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this might be true only for the data we are working with, and not truth in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>z13xxf3qlq2bxpm1o22zidpqbn2tfpcjr04</td>\n",
       "      <td>Connor Mire</td>\n",
       "      <td>2015-05-21 20:46:54.704</td>\n",
       "      <td>This Song is AWESOME!!!!﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>z13uzb3zqoylw1kl022zidpqbn2tfpcjr04</td>\n",
       "      <td>Connor Mire</td>\n",
       "      <td>2015-05-21 20:47:08.673</td>\n",
       "      <td>I&amp;#39;m A SUBSCRIBER﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              COMMENT_ID       AUTHOR                    DATE  \\\n",
       "810  z13xxf3qlq2bxpm1o22zidpqbn2tfpcjr04  Connor Mire 2015-05-21 20:46:54.704   \n",
       "809  z13uzb3zqoylw1kl022zidpqbn2tfpcjr04  Connor Mire 2015-05-21 20:47:08.673   \n",
       "\n",
       "                       CONTENT  CLASS  \n",
       "810  This Song is AWESOME!!!!﻿      0  \n",
       "809      I&#39;m A SUBSCRIBER﻿      1  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[test_df.AUTHOR == \"Connor Mire\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that *Connor Mire* posted both spam and ham. And the labels are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'77.30% of duplicate comments are spams.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spams, hams = duplicate_comments[\"CLASS\"].value_counts()\n",
    "f\"{(spams / (spams + hams)) * 100:.02f}% of duplicate comments are spams.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not need to transform emojis, some word embeddings handle them well:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">Will download <b>1.6 GB</b> large word embeddings model.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5047033597406592\n"
     ]
    }
   ],
   "source": [
    "vectors = Magnitude(MagnitudeUtils.download_model('fasttext/medium/wiki-news-300d-1M'))\n",
    "print(vectors.similarity(\":)\", \":D\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 0.0, 0.0, ..., 0.0, Timestamp('2015-05-20 03:50:29.098000'),\n",
       "        'you cant stop the shuffle\\ufeff']], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot-encode AUTHOR column\n",
    "pipeline = make_pipeline(\n",
    "    make_column_transformer(\n",
    "        (OneHotEncoder(), [\"AUTHOR\"]),\n",
    "        remainder=\"passthrough\",\n",
    "        sparse_threshold=0\n",
    "    )\n",
    ")\n",
    "# DEMO\n",
    "pipeline.fit(train_X, train_y)\n",
    "pipeline.transform(train_X[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible models and techniques to use:\n",
    "- [Bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model)\n",
    "- [Naive Bayes](https://www.kaggle.com/mohammedakhil/youtube-spam-filter-using-naive-bayes)\n",
    "- [LSTM](https://keras.io/api/layers/recurrent_layers/lstm/)\n",
    "- [DecisionTreeClassifier](https://datascience.stackexchange.com/questions/67250/decision-tree-in-sentiment-analysis)\n",
    "- [Magnitude Word Embeddings](https://colab.research.google.com/drive/1lOcAhIffLW8XC6QsKzt5T_ZqPP4Y9eS4#scrollTo=95Xg9EyU-ZYr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = load_train_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completely random: 0.44\n",
      "proportional: 0.50\n",
      "most frequent: 0.48\n"
     ]
    }
   ],
   "source": [
    "for strategy, strategy_name in zip([\"uniform\", \"stratified\", \"most_frequent\"],\n",
    "                                   [\"completely random\", \"proportional\", \"most frequent\"]):\n",
    "    dummy_clf = DummyClassifier(strategy=strategy)\n",
    "    dummy_clf.fit(train_X, train_y)\n",
    "    print(f\"{strategy_name}: {dummy_clf.score(test_X, test_y):.2f}\") # ignores the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words + Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count vect.; ComplementNB: 0.95\n",
      "Count vect.; ComplementNB - FP: 8, FN: 8\n",
      "Count vect.; BernoulliNB: 0.89\n",
      "Count vect.; BernoulliNB - FP: 4, FN: 29\n",
      "Count vect.; MultinomialNB: 0.94\n",
      "Count vect.; MultinomialNB - FP: 10, FN: 8\n",
      "TF-IDF vect.; ComplementNB: 0.94\n",
      "TF-IDF vect.; ComplementNB - FP: 8, FN: 10\n",
      "TF-IDF vect.; BernoulliNB: 0.89\n",
      "TF-IDF vect.; BernoulliNB - FP: 4, FN: 29\n",
      "TF-IDF vect.; MultinomialNB: 0.94\n",
      "TF-IDF vect.; MultinomialNB - FP: 10, FN: 10\n",
      "Count binary vect.; ComplementNB: 0.95\n",
      "Count binary vect.; ComplementNB - FP: 8, FN: 9\n",
      "Count binary vect.; BernoulliNB: 0.89\n",
      "Count binary vect.; BernoulliNB - FP: 4, FN: 29\n",
      "Count binary vect.; MultinomialNB: 0.94\n",
      "Count binary vect.; MultinomialNB - FP: 10, FN: 9\n",
      "Hashing vect.; ComplementNB: 0.91\n",
      "Hashing vect.; ComplementNB - FP: 21, FN: 7\n",
      "Hashing vect.; BernoulliNB: 0.91\n",
      "Hashing vect.; BernoulliNB - FP: 6, FN: 23\n",
      "Hashing vect.; MultinomialNB: 0.90\n",
      "Hashing vect.; MultinomialNB - FP: 24, FN: 7\n"
     ]
    }
   ],
   "source": [
    "N_FEATURES = len(CountVectorizer().fit(train_X).vocabulary_)*10\n",
    "\n",
    "vectorizers = [(\"Count vect.\", CountVectorizer()), \n",
    "               (\"TF-IDF vect.\", TfidfVectorizer()), \n",
    "               (\"Count binary vect.\", CountVectorizer(binary=True)),\n",
    "               (\"Hashing vect.\", HashingVectorizer(n_features = N_FEATURES, alternate_sign=False)),\n",
    "              ]\n",
    "bayeses = [\n",
    "    (\"ComplementNB\", ComplementNB()), \n",
    "    (\"BernoulliNB\", BernoulliNB()), \n",
    "    (\"MultinomialNB\", MultinomialNB())\n",
    "]\n",
    "\n",
    "for vectorizer_name, vectorizer in vectorizers:\n",
    "    for bayes_name, bayes in bayeses:\n",
    "        pipeline= make_pipeline(\n",
    "            vectorizer,\n",
    "            bayes\n",
    "        )\n",
    "\n",
    "        pipeline.fit(np.array(train_X), train_y)\n",
    "        y_pred = pipeline.predict(test_X)\n",
    "\n",
    "        print(f\"{vectorizer_name}; {bayes_name}: {pipeline.score(test_X, test_y):.2f}\")\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(test_y, y_pred).ravel()\n",
    "        print(f\"{vectorizer_name}; {bayes_name} - FP: {fp}, FN: {fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings + Keras LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 30 # he maximum number of words the sequence model will consider\n",
    "STD_DEV = 0.01 # Deviation of noise for Gaussian Noise applied to the embeddings\n",
    "HIDDEN_UNITS = 100 # The number of hidden units from the LSTM\n",
    "DROPOUT_RATIO = .8 # The ratio to dropout\n",
    "BATCH_SIZE = 100 # The number of examples per train/validation step\n",
    "EPOCHS = 50 # The number of times to repeat through all of the training data\n",
    "LEARNING_RATE = .01 # The learning rate for the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">Will download <b>1.6 GB</b> large word embeddings model.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = Magnitude(MagnitudeUtils.download_model('fasttext/medium/wiki-news-300d-1M'), pad_to_length = MAX_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = load_train_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GaussianNoise** - useful to mitigate overfitting (could be used as a form of random data augmentation). Gaussian Noise (GS) is a natural choice as corruption process for real valued inputs.\n",
    "As it is a regularization layer, it is only active at training time.\n",
    "\n",
    "**Bidirectional** – sequence processing model that consists of two LSTMs: one taking the input in a forward direction, and the other in a backwards direction. Bidirectional LSTMs effectively increase the amount of information available to the network, improving the context available to the algorithm\n",
    "\n",
    "**Dropout** – a stochastic regularization technique and should reduce overfitting by (theoretically) combining many different neural network architectures.\n",
    "With Dropout, the training process essentially drops out neurons in a neural network.\n",
    "\n",
    "**Dense** - The dense layer is a neural network layer that is connected deeply, which means each neuron in the dense layer receives input from all neurons of its previous layer.\n",
    "In the background, the dense layer performs a matrix-vector multiplication. The values used in the matrix are actually parameters that can be trained and updated with the help of backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(GaussianNoise(STD_DEV, input_shape=(MAX_WORDS, vectors.dim)))\n",
    "    model.add(Bidirectional(LSTM(HIDDEN_UNITS, activation='tanh'), merge_mode='concat'))\n",
    "    model.add(Dropout(DROPOUT_RATIO))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=Adam(lr=LEARNING_RATE),\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "36/36 [==============================] - 5s 73ms/step - loss: 0.6232 - accuracy: 0.6861 - val_loss: 0.8157 - val_accuracy: 0.6880\n",
      "Epoch 2/5\n",
      "36/36 [==============================] - 1s 30ms/step - loss: 0.3479 - accuracy: 0.8688 - val_loss: 0.2045 - val_accuracy: 0.9440\n",
      "Epoch 3/5\n",
      "36/36 [==============================] - 1s 32ms/step - loss: 0.2534 - accuracy: 0.9092 - val_loss: 0.1371 - val_accuracy: 0.9680\n",
      "Epoch 4/5\n",
      "36/36 [==============================] - 1s 32ms/step - loss: 0.2061 - accuracy: 0.9382 - val_loss: 0.1995 - val_accuracy: 0.9040\n",
      "Epoch 5/5\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.2509 - accuracy: 0.9134 - val_loss: 0.3353 - val_accuracy: 0.8400\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.3128 - accuracy: 0.8339\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8338658213615417"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = make_pipeline(\n",
    "    WordEmbeddingsSeries(vectors),\n",
    "    KerasClassifier(create_model, epochs=5, batch_size=32, validation_split=0.1)\n",
    ")\n",
    "\n",
    "clf.fit(train_X, train_y)\n",
    "clf.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/petr.janik/opt/anaconda3/envs/ib031/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[160   3]\n",
      " [ 49 101]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ham       0.77      0.98      0.86       163\n",
      "        Spam       0.97      0.67      0.80       150\n",
      "\n",
      "    accuracy                           0.83       313\n",
      "   macro avg       0.87      0.83      0.83       313\n",
      "weighted avg       0.86      0.83      0.83       313\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_pred_y = clf.predict(test_X)\n",
    "print_report(test_y, lstm_pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = load_train_test_all_cols_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC accuracy score :0.8785942492012779\n"
     ]
    }
   ],
   "source": [
    "svc = make_pipeline(\n",
    "    ExplorativeTransformer(),\n",
    "    FeatureSelector([\"SUSPICIOUS_WORDS_COUNT\", \"NULL_IN_DATE_TIME\", \"HAS_LINK\", \"NOT_UNIQUE_AUTHOR\"]),\n",
    "    SVC(),\n",
    ")\n",
    "\n",
    "svc.fit(train_X, train_y)\n",
    "svc_pred_y = svc.predict(test_X)\n",
    "print(\"SVC accuracy score :\" + str(accuracy_score(test_y, svc_pred_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[159   4]\n",
      " [ 34 116]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ham       0.82      0.98      0.89       163\n",
      "        Spam       0.97      0.77      0.86       150\n",
      "\n",
      "    accuracy                           0.88       313\n",
      "   macro avg       0.90      0.87      0.88       313\n",
      "weighted avg       0.89      0.88      0.88       313\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_report(test_y, svc_pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "* splitting the space by setting rules\n",
    "* removing the unnecessary splits\n",
    "* using the class with majority votes as the prediction\n",
    "\n",
    "Decision tree first splits the data based on these concepts: Pure and Impure, Impurity measurement, Information Gain.\n",
    "This is calculated when traversing trough the tree, then edge with more gain is used to decide about the categorization of an instance\n",
    "given to the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AddContainsCheckColumn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-509043acd833>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompose\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColumnTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m dtc = make_pipeline(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mAddContainsCheckColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mWordEmbeddingsDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"AUTHOR\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CONTENT\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mFeatureSelector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CONTAINS_CHECK\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype_include\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AddContainsCheckColumn' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "dtc = make_pipeline(\n",
    "    AddContainsCheckColumn(),\n",
    "    WordEmbeddingsDF([\"AUTHOR\", \"CONTENT\"]),\n",
    "    FeatureSelector(columns=[\"CONTAINS_CHECK\"], dtype_include=\"float32\"),\n",
    "    Debugger(),\n",
    "    DecisionTreeClassifier()\n",
    ")\n",
    "\n",
    "dtc.fit(train_X, train_y)\n",
    "print(f\"DTC accuracy score: {dtc.score(test_X, test_y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_pred_y = dtc.predict(test_X)\n",
    "print_report(test_y, dtc_pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "**Baseline MNBayes** - 0.9–0.95\n",
    "\n",
    "**LSTM** - 0.93\n",
    "\n",
    "This model uses Magnitude library to convert words into vectors of numbers (word embeddings). This conversion, combined with LSTM, preserves the order of words.\n",
    "However, MLP with CountVectorizer, which does not preserve the order performed slightly better (0.95). This iundicates that the architecture and hyperparameters of the used LSTM still need some fine tuning.\n",
    "\n",
    "**SVC** - 0.87\n",
    "\n",
    "This result was reached with our state-of-the-art ExplorationTransformer. The SVM classifier was also tried with a Count Vectorizer and, interestingly, it performed better with stop words included than without them (0.94 vs 0.92).\n",
    "\n",
    "**Tree Classifier** - 0.95\n",
    "\n",
    "Considering the relative simplicity of this model, it performed rather well on the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have prepared many transformers, which could be used to create new columns. \n",
    "We could then try different combinations of these columns and select the best, perhaps by using SelectKBest.\n",
    "\n",
    "However, considering the success of even the baseline model, and the success of other models on just the vectorized comments, the performance on this\n",
    "dataset might increase only marginally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ib031] *",
   "language": "python",
   "name": "conda-env-ib031-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
